---
footer: concentrations
---

We discuss in this section a few basic concentration inequalties, Chernov bounds, Hoeffding bounds and McDiarmid bounds
and give some applications. Concentration inequalties are a very important tools to provide performance guarantees for Monte-Carlo methods, but also in a variety of other contexts in Uncertainty Quantification and Machine Learning. For further reading we recommend the excellent book [@Boucheron2013]. 


## Chernov bounds
 
We start with a basic but fundamental bound 
 
:::{#thm-MarkovChernov name="Markov and Chernov inequalities"}

1. **Markov inequality**:  Suppose $Y\ge 0$ is a non-negative random variable and $a>0$ a positive number. Then 
$$
 P(Y \ge a) \le \frac{E[Y]}{a}\,.
$${#eq-Markov}       
      
1. **Chernov inequality**. Suppose $X$ is a random variable and $a \in \mathbb{R}$ then 
$$
\begin{aligned}
& P(X \ge a)  \le \inf_{t \ge 0} \frac{E[e^{tX}]}{e^{ta}} \\
& P(X \le a)  \le \inf_{t \le 0} \frac{E[e^{tX}]}{e^{ta}}
\end{aligned}
$${#eq-Chernov}    

::: 

. . .

:::{.proof}  


If $Y\ge 0$ then $\displaystyle Y=Y 1_{\{Y\ge a\}} + Y1_{\{Y\le a\}} \ge Y1_{\{Y\ge a\}} \ge a 1_{\{Y\ge a\}}$ and thus $\displaystyle E[Y]  \ge a P(Y\ge a)$ which is @eq-Markov.   
Using Markov inequality we have for $t \ge 0$ 
  $$
  P(X \ge a) = P\left( e^{tX} \ge e^{ta} \right) \le \frac{E[e^{tX}]}{e^{ta}} \,.
  $$ 
Optimizing over $t$ gives the first inequality in @eq-Chernov. The second  inequality is obtained by replacing $X$ by $-X$.
:::

## Examples of Chernov bounds

+ If $X$ is  normal with mean $\mu$ and varaince $\sigma^2$, then $M(t)=e^{\mu t + \frac{\sigma^2 t^2}{2}}$ and, 
for $a = \mu + \varepsilon$, Chernov gives
$$
P(X \ge \mu + \varepsilon) \le \inf_{t \ge 0} \frac{e^{\mu t + \frac{\sigma^2 t^2}{2}} }{e^{t(\mu + \varepsilon)}} 
= e^{ -\sup_{t\ge 0}\left\{ t\varepsilon - \frac{\sigma^2 t^2}{2}\right\} } = e^{-\frac{\varepsilon^2}{2\sigma^2}}
$$
This is sharp: one can show (see exercises) that 
$$
\left(\frac{1}{\varepsilon}-\frac{1}{\varepsilon^3\sigma^2} \right)\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{\varepsilon^2}{2\sigma^2}} \le P(X \ge \mu + \varepsilon) \le \frac{1}{\varepsilon}\frac{1} {\sqrt{2\pi}\sigma} e^{-\frac{\varepsilon^2}{2\sigma^2}}
$$

. . .

+ Consider $X_n$ is binomial with parameters $(n,p)$ then $M(t)=\left(pe^{t}+(1-p)\right)^n$. We will use that 

$$
\sup_{t} \left\{ tz - \ln(pe^t + (1-p)) \right\} = 
\left\{
\begin{array}{cl}
+ \infty & \textrm{ if } z<0 \textrm{ or } z>1 \\
z \ln \frac{z}{p} + (1-z) \ln \frac{1-z}{1-p} := H(z||p) & \textrm{ if } 0 \le z \le 1 
\end{array}
\right.
$$
$H(z||p)=z \ln \frac{z}{p} + (1-z) \ln \frac{1-z}{1-p}$ is the KL-divergence or (relative entropy) between Bernoulli distributions with paramters $z$ and $p$ respectively.  

Chernov bound gives 
$$
P\left(X_n \ge n(p+\varepsilon)\right) \le \inf_{t >0} \frac{(pe^t + (1-p))^n}{e^{tn(p+\varepsilon)}}= e^{-n \sup_{t\ge 0}\left\{ t(p+\epsilon)-\ln(pe^t+(1-p)) \right\}} = e^{-n H( p+\varepsilon||p)} 
$$
and similarly $P\left(X_n \le n(p-\varepsilon)\right) \le e^{-n H( p-\varepsilon||p)}$



## Hoeffding's bound

+ Chernov bounds are very sharp but requires knowledge of the mgf $M_X(t)$. 

. . .

+ One of the main idea behind concentration inequalties: given $X$ bound $M_X(t) \le M(t)$ by the mfg of a random variable $Y$ which you know explcitly.  Mostly here we take $Y$ a Gaussian but one also uses other, Bernoulli, Poisson, Gamma, etc...


. . .

+ The following elementary bound will be used repeatedly.

:::{#lem-Hoeffdingsbound name="Hoeffding's bound"} 
Suppose $a \le X \le b$ with probability $1$. Then for any $\varepsilon > 0$

1. Bound on the variance $\displaystyle \textrm{Var}(X) \le \frac{(b-a)^2}{4}$ 

2. Bound on the mgf $\displaystyle E\left[ e^{tX} \right] \le e^{t E[X]} e^{\frac{t^2 (b-a)^2}{8}}$

:::

:::{.proof}

For the bound on the variance $a \le X \le b$ implies that $\displaystyle -\frac{a-b}{2} \le X - \frac{a+b}{2} \le \frac{a-b}{2}$ 
and therefore 
$$
\textrm{Var}(X) = \textrm{Var}\left(X - \frac{a+b}{2}\right) \le E\left[\left(X - \frac{a+b}{2}\right)^2\right] \le \frac{(b-a)^2}{4}\,.
$$
:::

## {-}

Since $X$ is bounded the moment generating function $M(t) =\frac{e^{tX}}{E[e^{tX}]}$ exists for any $t \in \mathbb{R}$. 
To bound the $M(t)$ let us consider instead its logarithmx $u(t)=\ln M(t)$. We have 
$$
\begin{aligned}
u'(t) &= \frac{M'(t)}{M(t)}  &=& E\left[ X \frac{e^{tX}}{E[e^{tX}]}  \right] \\
u''(t) &= \frac{M''(t)}{M(t)}- \left(\frac{M'(t)}{M(t)}\right)^2  &=&    E\left[ X^2 \frac{e^{tX}}{E[e^{tX}]}  \right] - E\left[ X \frac{e^{tX}}{E[e^{tX}]}  \right]^{2}
\end{aligned}
$$
We recognize $u''(t)$ as the variance under the tilted measure with tilted density $\frac{e^{tX}}{E[e^{tX}]}$ and thus by part 1.
(applied to the tilted measure) we have $u''(t) \le \frac{(b-a)^2}{4}$.  
Using the Taylor expansion with remainder we have, for some $\xi$ between $0$ and $t$ 
$$
\ln M(t)= u(t) = u(0) + u'(0)t + u''(\xi) \frac{t^2}{2} \le t E[X] + \frac{t^2(b-a)^2}{8}\,. 
$$
This concludes the proof. 

. . .

**Remark:** The bound on the variance in 1. is optimal. Indeed taking without loss of generality $a=0$ and $b=1$ then the variance is bounded by $1/4$ and this realized by taking $X$ to be a Bernoulli with $p=\frac12$. This bound says that the RV with the largest variance is the one where the mass is distributed at the end point.  

. . .

The bound in 2. is optimal only in the sense that it is the best *quadratic* bound on $u(t)$. For example for a Bernoulli with $a=0$ and $b=1$ we have $M(t)=\ln (\frac{1}{2}e^{t} + \frac{1}{2})= \frac{1}{2}t + \ln \cosh\left(\frac{t}{2}\right)$ which is much smaller (for large $t$). There is room sometimes for better bounds (see e.g. [@Boucheron2013]) but ising Gaussian is computationally convenient. 


## Hoeffding's theorem


:::{#thm-hoeffding name="Hoeffding's Theorem"}
Suppose $X_1, \cdots, X_N$ are independent random variables such that  $a_i \le X \le b_i$ (almost surely). Then 

$$
\begin{align}
P \left( X_1 + \cdots + X_n - E[X_1 + \cdots + X_n]  \ge \varepsilon \right)
&\le    e^{- \frac{2 \varepsilon^2}{\sum_{i=1}^n(b_i-a_i)^2}}\\
 P \left( X_1 + \cdots + X_n - E[X_1 + \cdots + X_n]  \le -\varepsilon \right)
&\le   e^{- \frac{2 \varepsilon^2}{ \sum_{i=1}^n(b_i-a_i)^2}}
\end{align}
$${#eq-hoeffding}
:::

. . .

:::{.proof}
Using independence the Hoeffding's bound we have 
$$
e^{t (X_1 + \cdots + X_n - E[X_1 + \cdots + X_n])} =\prod_{i=1}^n e^{t(X_i-E[X_i])} \le \prod_{i=1}^n e^{\frac{t^2 (b_i-a_i)^2}{8}} = e^{ \frac{t^2 \sum_{i}(b_i-a_i)^2}{8}}
$$
and using Chernov bound (for a Gaussian RV with variance $\displaystyle \sum_{i}\frac{(b_i-a_i)^2}{4}$) gives the first bound in @eq-hoeffding.  
::: 

. . .

:::{#cor-confidence name="non-asymptotic confidence interval"} 
Suppose $X_1, \cdots, X_N$ are independent random variables such that  $a \le X \le b$ (almost surely) and $\mu=E[X_i]$. 

$$
P\left( \mu \in \left[ \frac{S_N}{N}-\varepsilon, \frac{S_N}{N} +\varepsilon \right]  \right) \ge 1- 2 e^{-\frac{2 N \varepsilon^2}{(b-a)^2}}  
$$

:::


## {-} 


We can rewrite this as 

$$
P\left( \mu \in \left[ \frac{S_N}{N}- \sqrt{ \frac{(b-a)^2\ln\left(\frac{2}{\delta}\right)}{2N}}, \frac{S_N}{N} +\sqrt{ \frac{(b-a)^2\ln\left(\frac{2}{\delta}\right)}{2 N}} \right]  \right) \ge 1 -\delta
$$

+ Comparison to asymptotic confidence interval, take $X_i$ to be Bernoulliwith unknown $\mu=p$ so we bound the variance by 
$p(1-p) \le \sigma_{max}=\frac{1}{4}$ (this is what we did in Hoeffding with $a=0,b=1$. So we have 



$$
(\textrm{CLT}) \quad z_{1-\delta} \frac{\sigma_{max}}{\sqrt{N}} \quad \textrm{ vs } \quad \sqrt{2\ln\left(\frac{2}{\delta}\right)}   \frac{\sigma_{max}}{\sqrt{N}} \quad (\textrm{Hoeffding})
$$




## McDiarmid theorem

A very useful extension of Hoeffding's applies to suitable function $h(X_1, X_2, \cdots, X_n)$ other than just the sum. 

We say that $h(x_1, x_2, \cdots, x_n)$ satisifies the **bounded difference property** if there exist constants $c_k$ such that 
for all $x_k, x_k'$
$$
|h(x_1,\cdots, x_k, \cdots, x_n) - h(x_1,\cdots, x_k', \cdots, x_n)| \le c_k
$$

. . .


:::{#thm-McDiarmid name="McDiarmid Theorem"}
Suppose $X_1, \cdots X_n$ are independent RVs and $f(x_1, \cdots, x_n)$ satisfies the bounded difference property (almost surely).
Then we have 
$$
\begin{aligned}
P\left( h(X_1,\cdots, X_n) - E[h(X_1, \cdots, X_n)] \ge \varepsilon \right)  \le &  e^{- \frac{\varepsilon^2}{2\sum_{k=1}^n c_k^2}} \\
P\left( h(X_1,\cdots, X_n) - E[h(X_1, \cdots, X_n)] \le -\varepsilon \right)  \le & e^{- \frac{\varepsilon^2}{2\sum_{k=1}^n c_k^2}}
\end{aligned}
$$

:::

. . .

:::{.proof} 
The proof use the basic properties of conditional expectation. 

**"Martingale trick"**: Let us define random variable $Y_k$ by $Y_0=E[f(X_1, \cdots, X_n)]$ and, for $1\le k \le n$, 
$$
Y_k =E[h(X_1,\cdots, X_n)|X_1, \cdots, X_k]
$$
Note that $Y_n=h(X_1,\cdots, X_n)$ and also $E[Y_k| X_1, \cdots, X_{k-1}]= Y_{k-1}$ (the martingale property). 
If we define further $Z_k=Y_k-Y_{k-1}$ then we have the telescoping sum 
$h(X_1,\cdots, X_n) - E[h(X_1, \cdots, X_n)] = \sum_{k=1}^n Z_k$
and $E[Z_k|X_1,\cdots,X_{k-1}]=0$.

:::

## {-}


**Duplication trick:** Let $\widehat{X}_k$ be an independent copy of the random variable $X_k$.  Then by the bounded difference property we have, almost surely,  
$$
\left| E[h(X_1, \cdots, X_k, \cdots, X_n)|X_1, \cdots, X_k] - E[h(X_1, \cdots, \widehat{X}_k, \cdots, X_n)|X_1, \cdots, X_k]\right| \le c_k
$${#eq-bdf}
Furthermore we have 
$$
\begin{aligned}
E[h(X_1, \cdots, X_k, \cdots, X_n)|X_1, \cdots, X_{k-1}]&=E[h(X_1, \cdots, \widehat{X}_k, \cdots, X_n)|X_1, \cdots, X_{k-1}]\\
&=E[h(X_1, \cdots, \widehat{X}_k, \cdots, X_n)|X_1, \cdots, X_K]
\end{aligned}
$${#eq-duptrick}
The first equality holds because $X_k$ and $\widehat{X}_{k}$ are identically distributed and left-hand side is a function of $X_1, \cdots, X_{k-1}$. The second equality holds because $X_k$ and $\widehat{X}_{k}$ are idependent.  Combining @eq-bdf and @eq-duptrick
shows that $|Z_k| \le c_k$ almost surely.   

**Conditioning and Hoeffding:** We have, using conditioning and Hoeffding's bound from @lem-Hoeffdingsbound
$$
\begin{aligned}
E\left[ e^{t\sum_{k=1}^n Z_k} \right] &= E \left[ E \left[ e^{t\sum_{k=1}^{n-1} Z_k} e^{tZ_n} | X_1, \cdots X_{n-1}\right] \right]
= E \left[  e^{\sum_{k=1}^{n-1} Z_k} E \left[  e^{Z_n} | X_1, \cdots X_{n-1}\right] \right] \le e^{\frac{t^2 c_n^2}{2}} E \left[  e^{t \sum_{k=1}^{n-1} Z_k}\right]
\end{aligned}
$$
where we used that $E[Z_n|X_1, \cdots, X_{n-1}]=0$ and $\textrm{ Var}(Z_n|X_1, \cdots, X_{n-1}) \le c_n^2$. By induction we find

$$
E\left[ e^{t h(X_1, \cdots, X_n) -E[h(X_1, \cdots, X_n)]} \right] \le e^{\frac{t^2 \sum_{k=1}^n c_k^2}{2}}
$$
and the rest follows as usual. 

## Confidence interval for variance estimator

We show how to build confidence interval for the unbiased variance estimator 
$$
V_N(X_1, \cdots, X_N)=\frac{1}{N-1} \sum_{k=1}^N\left(X_i - \frac{S_N}{N}\right)^2 = \frac{1}{N-1}\sum_{i=1}^N X_i^2 - \frac{S_N^2}{(N-1)N}
$$
If we change $X_1$ into $\widehat{X}_1$ then 
$S_N(X_1, \cdots, X_N ) - S_N(\widehat{X}_1, \cdots, X_N ) = X_1 - \widehat{X}_1$
and so 
$$
V_N(X_1, \cdots, X_N)- V_N(\widehat{X}_1, \cdots, X_N) = \frac{X_1^2 - \widehat{X}_1^2}{N-1}  - \frac{X_1 - \widehat{X}_1}{N-1} \left(\frac{S_N(X_1, \cdots, X_N )}{N} + \frac{S_N(\widehat{X}_1, \cdots, X_N )}{N} \right)
$$
So if we assume $|X_i|\le c$ we have the bound 

$$
|V_N(X_1, \cdots, X_N)- V_N(\widehat{X}_1, \cdots, X_N)| \le \frac{5c^2}{N-1} 
$$
and thus by McDiarmid we get 
$$
P( V_N - \sigma^2 \ge \varepsilon ) \, \le \, e^{ - \frac{(N-1)^2}{N}\frac{\varepsilon^2}{50c^4}} \,\le\, e^{ -(N-2)\frac{\varepsilon^2}{50c^4}}
$$
and this decay exponentially in $N$ again and can be used for a confidence interval for the variance which is very similar to the one for the mean. 

## Bernstein bound

In Hoeffding's bound we use, in an essential way, a bound on the variance. If the variance is small then one should expect the bound to be poor. Bernstein bound, which can be used if we have some a-priori knowledge about the variance, improves. 

:::{#thm-bernsteinbound name="Bernstein Bound"}
Suppose $X$ is a random variable such that $|X-E[X]| \le c$ and $\textrm{var}(X) \le \sigma^2$. Then 
$$
E[e^{t X}]\le e^{t E[X]+ \frac{\sigma^2 t^2}{2(1 -c|t|/3)}} \,.
$$
:::


:::{.proof} 


We expand the exponential and use that for $k\ge 2$, 
$$
E\left[(X-\mu)^k\right] \le E\left[(X-\mu)^2|X-\mu|^{k-2}\right]\le  E[(X-\mu)^2] c^{k-2} \le \sigma^2 c^{k-2}  
$$
and get 
$$
\begin{aligned}
E\left[e^{t(X-\mu)}\right]=1+ \sum_{k=2}^\infty \frac{t^k}{k!} E[(X-\mu)^k] &\le 1 + \frac{t^2\sigma^2}{2}\sum_{k=2}\frac{2}{k!}(|t|c)^{k-2} \\
&\le 1 + \frac{t^2\sigma^2}{2}\sum_{k=2}^\infty \left(\frac{|t|c}{3}\right)^{k-2} \quad \textrm{ since } \frac{k}{2!}\ge 3^{k-2}\\
&\le 1 + \frac{t^2\sigma^2}{2(1-\frac{|t|c}{3})} \le e^{\frac{t^2\sigma^2}{2(1-\frac{|t|c}{3})}} \quad \textrm{ since } 1+ x \le e^x 
\end{aligned}
$$
and this concludes the proof.

:::

## Bernstein confidence interval

To obtain a confidence interval we need to solve an optimization problem: after some straightforward computation we find 

$$
\sup_{t \ge 0} \left\{ \varepsilon t - \frac{a t^2}{2(1-bt)} \right\} = \frac{a}{b^2}h\left( \frac{b\epsilon}{a}\right) \quad \textrm{ where } h(u) = 1 +u - \sqrt{1 + 2u}
$$
and we note that the $h^{-1}(z)= z + \sqrt{2z}$.  We obtain the exact same bound for the left tail (by symmetry) and thus  
to obtain a confidence interval we have 
$$
\delta = 2 e^{-\frac{a}{b^2}h\left( \frac{b\varepsilon}{a}\right)} \iff  \varepsilon = b \ln\left( \frac{2}{\delta}\right)  + \sqrt{2a \ln\left( \frac{2}{\delta}\right)}
$$

:::{#thm-BernsteinIID name="Bernstein for sum of IID"}

If $X_1, \cdots, X_N$ are IID random variables with $|X_i| \le c$ and $\textrm{Var}(X_i)\le \sigma^2$ then 

$$
P\left( \mu \in \left[\frac{S_N}{N} - \frac{c}{3N} \ln\left( \frac{2}{\delta}\right)  - \sqrt{\frac{2\sigma^2}{N} \ln\left( \frac{2}{\delta}\right)}, \frac{S_N}{N} + \frac{c}{3N} \ln\left( \frac{2}{\delta}\right)  + \sqrt{\frac{2\sigma^2}{N} \ln\left( \frac{2}{\delta}\right)}   \right] \right) \ge 1 - \delta
$$


:::

:::{.proof} 
This is Bernstein bound with $a=N \sigma^2$ and $b=c/3$ and $\varepsilon \to N \varepsilon$. 
:::


$$
(\textrm{Bernstein}) \quad \frac{c}{3N} \ln\left( \frac{2}{\delta}\right)  + \sqrt{2 \ln\left( \frac{2}{\delta}\right)} \frac{\sigma}{\sqrt{N}} \quad \textrm{ versus} \quad   \sqrt{2\ln\left(\frac{2}{\delta}\right)}   \frac{\sigma_{max}}{\sqrt{N}} \quad (\textrm{Hoeffding})
$$

You should note that this bound can be substantially better than Hoeffding's bound  is $\sigma \le \sigma_{max}$


## Comparing the bounds for Bernoulli 

Hoeffdings amount to bounding  the variance by $\frac14$. Bernstein which incorporporates knowledge of (or a bound on) the variance is much better than Hoeffdings as soon as the variance is not too big ($0.1$ in the epicture below.)


```{python}

import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt

delta, var_max, var_bound, c =0.01, 0.25, 0.1, 1
N=np.linspace(1000,100000,50000)
epsilon_Hoeffding = np.sqrt(2*np.log(2/delta))*(var_max/np.sqrt(N)) 
epsilon_Bernstein = np.sqrt(2*np.log(2/delta))*(var_bound/np.sqrt(N)) + (c/3)*np.log(2/delta)*(1/N)
epsilon_CLTmax = norm.ppf(1-(delta/2))*(var_max/np.sqrt(N))  
epsilon_CLTbound = norm.ppf(1-(delta/2))*(var_bound/np.sqrt(N))

plt.figure(figsize=(4,3))
plt.plot(N,epsilon_Hoeffding,color='r',label='Hoeffding')
plt.plot(N,epsilon_Bernstein,color='b',label='Bernstein')
plt.plot(N,epsilon_CLTmax,color='g',label='CLT (max variance)')
plt.plot(N,epsilon_CLTbound,color='purple',label='CLT (variance bound)')
plt.xlabel('$N$')
plt.ylabel('$\epsilon$')
plt.title('CLT vs Hoeffding vs Bernstein')
plt.legend()
plt.show()

```


