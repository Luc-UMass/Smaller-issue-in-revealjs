

We discuss two examples on how to reduce the variance of an algorithm. This is a vast subject and there are many other technique to perform  *variance reduction* e.g. conditional MC, control variates, stratified sampling, etc..... Some of them will appear in the exercises. Recommended readings are [@Ross2019], [@Ross2013], [@Rubinstein2017] where you will find much more information.


## Antithetic random variables and common random numbers


 + The idea is to use  *dependent random variables*  or dependent random numbers to decrease the
variance.  Recall the formula for variance: for  random variables $X$ and $Y$ we have
$$
\textrm{Var}(X+Y) = \textrm{Var}(X)+ \textrm{Var}(Y)+ 2 \textrm{Cov}(X,Y) \,.
$$
If $X$ and $Y$ are *negatively correlated*, i.e. $\textrm{Cov}(X,Y) < 0$ then we can possibly reduce the variance. 

. . . 


+ To build up our intuition consider the random variables $U$ and $V=1-U$, they are identically distributed 
and negatively correlated because if $U \le \frac{1}{2}$ then $V \ge \frac{1}{2}$ and vice versa:
$$
\left(U - \frac{1}{2}\right)\left((1-U)-\frac{1}{2}\right) = U(1-U) - \frac{1}{4} \le 0 \quad \textrm{ since }  \max_{x} x(1-x) = \frac{1}{4}
$$

. . .

+ The key idea for the general case is contained in the following theorem

:::{#thm-antithetic}  

Suppose $X_1, \cdots, X_n$ are independent random variables and $f$ and $g$ are increasing function of $n$ variables.  Then
$$
\textrm{Cov}( f(X_1, \cdots X_n) , g(X_1, \cdots X_n)) \ge 0 \,.
%=  E[f(X_1, \cdots X_n) g(X_1, \cdots X_n)] - E[f(X_1, \cdots X_n)] E[ g(X_1, \cdots X_n)] \ge 0
$${#eq-cov:positive}

:::


## {-}

:::{.proof} 
By induction. For $n=1$, since $f$ and $g$ are increasing, for any $x,y$ we have 
$0 \le (f(x)-f(y))(g(x)-g(y))$
and thus for any random variable  $X$, $Y$ 
$$
0 \le E [ f(X)-f(Y))(g(X)-g(Y)) ]  = E [f(X)g(X)]+ E[ f(Y)g(Y) ] - E [f(X)g(Y)] + E [f(Y)g(X)] 
$$
If, in addition, $X$ and $Y$ are IID we get 
$$
0 \le 2 E[f(X) g(X)] - 2 E[f(X)] E[g(X)] = 2 \textrm{Cov}(f(X),g(X))
$$
which proves @eq-cov:positive for the case for $n=1$.

Assuming now that @eq-cov:positive is true for $n-1$ we condition on the value of $X_n$ and

$$
\begin{aligned}
E[ f( X_1, \cdots X_n) g (X_1, \cdots X_n) \vert X_n= x_n] & = E[ f(X_1, \cdots, X_{n-1}, x_n ) g ( X_1, \cdots X_{n-1}, x_n)]   \quad \textrm {by independence}   \\
&  \ge  E[ f(X_1, \cdots, X_{n-1}, x_n )] E[ g ( X_1, \cdots X_{n-1}, x_n)]  \quad \textrm {by induction}  \\
 &= E[ f(X_1, \cdots, X_n )\vert X_n= x_n] E[ g ( X_1, \cdots, X_n)\vert X_n= x_n]
\end{aligned}
$$ 

Taking expectation on both sides one finds
$$
 E[ f( X_1, \cdots, X_n) g (X_1, \cdots, X_n)] 
 \ge E\big[  E[ f( X_1, \cdots X_n) \vert X_n] E[ g (X_1, \cdots X_n) \vert X_n] \big] 
$$
Note that the functions $E[ f( X_1, \cdots X_n) \vert X_n]$  and $E[ g( X_1, \cdots X_n) \vert X_n]$  are increasing in $X_n$, so @eq-cov:positive for $n=1$, 

$$
\begin{aligned}
   E\big[   E[ f( X_1, \cdots X_n) \vert X_n] E[ g (X_1, \cdots X_n) \vert X_n] \big]  
& \ge    E\big[   E[ f( X_1, \cdots X_n) \vert X_n] \big]  E\big [E[ g (X_1, \cdots X_n) \vert X_n] \big ] \\
 &  =  E[ f( X_1, \cdots X_n)] E[g( X_1, \cdots X_n)]
\end{aligned}
$$
 and this proves the theorem. \qed
:::




## A simple example

+ Compute $\mu = E\left[ e^U \right]$ use MC (of course the answer is $e-1$...). 

+ Since $h(x)=e^x$ is increasing and $g(x)=e^{1-x}$ is decreasing we can use antithetic RV.

+ We have the covariance
$$
\textrm{Cov}\left(e^U, e^{1-U}\right)=E\left[e^U e^{1-U}\right]-E\left[e^U\right] E\left[e^{1-U}\right]=e -(e-1)^2=-0.2342
$$
+ We also have 
$$
\textrm{Var}\left(e^U\right)= E\left[e^{2U}\right]- E\left[e^U\right]^2 = \frac{e^{2}-1}{1}-(e-1)^2=0.2420
$$

+ To compare: 

  + Independent random numbers $U_1,U_2$ give the variance
  $$
  \textrm{Var}\left( \frac{e^{U_1}+e^{U_2}}{2} \right) = \frac{ \textrm{Var}\left(e^{U}\right)}{2}= 0.1210
  $$
  
  + Antithetic random variables $U, 1-U$ give the variance
  $$
  \textrm{Var}\left( \frac{e^{U_1}+e^{U_2}}{2} \right) = \frac{ \textrm{Var}\left(e^{U}\right)}{2} + \frac{\textrm{Cov}\left(e^U, e^{1-U}\right) }{2}=.0039
  $$

+ A variance reduction of 96.7 percent (not bad...)


## Reliability function 

+ A system consists of $n$ components, each of which is either works or fails. We set
$$
x_i= \left\{  \begin{array}{cl}  1 & \textrm{ if component } i \textrm { works} \\
0 & \textrm{ if component } i \textrm { fails} 
\end{array}
\right. \,.
$$
We call $\mathbf{x}=(x_1, \cdots, x_n)$ the state vector describing the state of the system. 

. . .

+ A *structure function* $h(x_1, \cdots, x_n)$ describe the state of the system  
$$
h(\mathbf{x})= \left\{  \begin{array}{cl}  1 & \textrm{ if the system is functioning} \\
0 & \textrm{ if the system fails}
\end{array}
\right. \,,
$$
It is natural to assume that *$h$ is an increasing function of $x_i$* (turning a system on cannot make the system fail)

. . .

+ Probabilistic description: assume that the *components are independent* and are described a Bernoulli random variable $X_i$ with 
$$
p_i = P( X_i=1 ) = P( \textrm{ component } i \textrm{ works}) = 1- P( \textrm{ component } i \textrm{ fails})
$$

+ The RV $h(\mathbf{X})=h(X_1, \cdots, X_n)$ is also Benoulli and we have 
$$
E[h(\mathbf{X})] = P(\textrm{ system works })
$$
and we may want to use Monte-Carlo

## {-}

**Examples of systems**


+ The *series structure*: $h(x_1,\cdots,x_n)=\min_{i} x_i$ (all components must work) 

+ The *parallel structure*: $h(x_1,\cdots,x_n)=\max_{i} x_i$ (at least one component must work)

+ The *$k$-out-of-$n$ structure*: $h(x_1,\cdots,x_n)= \left\{ \begin{array}{cl} 1 &\textrm{if }\sum_{i=1}^n x_i\ge k\\0 & \textrm{otherwise}  \end{array} \right.$ (at least $k$ components must work)

+ The *bridge structure*:  $h(x_1,x_2,\cdots, x_5) = \max \{ x_1 x_3 x_5, x_2 x_3 x_4,x_1 x_4, x_2 x_5 \}$



::: {#fig-structures layout-ncol=4}

![series](images/S-series){width="250" }

![parallel](images/S-parallel){width="250"}

![2 out of 3](images/S-2-of-3){width="250"}

![bridge](images/S-bridge){width="250"}

Example of structures
:::

## Stucture function


General principled way to compute the structure function

  + A *path* $A$ is a subset $A \subset \{1,\cdots, n\}$  such that $h(\mathbf{x})=1$ if $x_i=1$ for all $i\in A$, that is it a subset of components which will ensure that the system works.

  + A *minimal path* $A$ is a path $A$ such that no subset of $A$ is itself a path, that is it a minimal subset of components which will ensure that the system works.

  

:::{#thm-structure.function }  
Using the notation $x_A = \prod_{i\in A} x_i$ for any subset $A \subset \{1,\cdots,n\}$, the 
the structure function for an arbitrary system is given by 

$$
\begin{aligned}
h(\mathbf{x}) & =  \max_{A \,\,\textrm{minimal path}}  X_A \\
              & =  1 -  \prod_{A \textrm{ minimal path}}  (1 -  X_A) 
\end{aligned}
$$


:::

:::{.proof} 
For the system to work we need to have all components working for at least one minimal path $A$ and so $X_A=1$. 
If this is the case $\max_{A \,\,\textrm{minimal path}}  X_A=1$ and $\prod_{A \textrm{ minimal path}}  (1 -  X_A)=0$. $\quad \blacksquare$
:::

The theorem is of limited usability since it requires enumeration of all minimal paths. If the system is moderately big, this is very difficult and/or tedious.

## Monte-Carlo for structure functions

+ If we assume a probabilistic model, which is pretty reasonable. Think for example, of a routing network, where each node have a certain probability of failure.

:::{.algorithm}
**MC for system reliability**: Consider a system with stucture function $h(\mathbf{x})$ and assume the components $X_k$, $k=1,\cdots, n$ are independent Bernoulli RB with with $P(X_k=1)=p_k$. To compute $E[h(\mathbf{X})]=P(\textrm{system works})$  
 
1.Generate nN independent random numbers $U_{ik}$ with $1\le i \le N$ and $1\le k \le n$ and set $X_{ik}=\mathbf{1}_{U_{ik}\le p_k}$.

2. Evaluate $H(\mathbf{X_i})$ where $\mathbf{X_i}=(X_{i1}, \cdots, X_{in})$ 

2. Set $I_N \,=\, \frac{1}{N}\sum_{k=1}^N H(\bf X_i)$ is an estimator for $P(\textrm{system works})$.

It is important to note that often it easier to evaluate $H( {\bf X})$ (that is check if the system functions) rather than evaluate the  structure function (well-known algorithm can be used).
::: 

+ The reliability function $g(U_1, \cdots, U_n)= h\left( \mathbf{1}_{\{U_1\le p_1\}}, \cdots,  \mathbf{1}_{\{U_n\le p_n\}} \right)$ is an increasing function so 
$$
\textrm{Cov}(g(U_1, \cdots, U_n),g(1-U_1, \cdots, 1-U_n) ) <0 
$$
  and we can reuse each random number and reduce the variance.

## Importance sampling  

+ Consider continuous RVs but the same ideas works for discrete RVs.  Suppose $X$ has pdf  $f(x)$ and we want to compute $\mu= E[h(X)] = \int h(x) f(x) dx$. One can always use the simple MC algorithm  $I_N =\frac{1}{N} \sum_{k=1}^N h(X_k)$.

. . . 

+  The idea behind  *importance sampling* is to sample from another random variable $Y$ with density
$g$ and write
$$
\mu= E[h(X)] = \int h(x) f(x) dx = \int h(x) \frac{f(x)}{g(x)} g(x) dx = E\left[ \frac{h(Y) f(Y)}{g(Y)}\right] \,.
$$
For the integral to make sense we  need to pick $g$ such that $g(x) = 0$  is allowed only if $f(x)=0$  or $h(x)f(x) =0$.

. . .


+  Often in practice if $X$ belongs to some family, say an exponential RV, we will pick $Y$ from the same family, say an exponential or a gamma random variable but with other paramters which we should pick carefully. 


:::{.algorithm}
**Importance sampling  Monte-Carlo estimator**:
To  estimate $\mu = E\left[ h(X) \right]$ the importance sampling algorithm use  the random variable $Y$ with pdf $g$ and has the estimator
$$
J_N = \frac{1}{N} \sum_{k=1}^{N} \frac{h(Y_k) f(Y_k)}{g(Y_k)}
$$
:::

. . .

+ Notice the similarity (in spirit) with rejection method. 




## {-}

+ How to pick $g$?   

. . .
 
+ The variance of the simple MC estimator $I_N$ is 
$\textrm{Var}(I_N) = \frac{1}{N} ( E[ h(X)^2] - E[h(X)]^2)$. 

. . .

+ For the importance sampling estimator $J_N$
we have 
$$
\begin{aligned}
\textrm{Var}(J_N) =\frac{1}{N} \textrm{Var}\left( \frac{ h(Y) f(Y)}{g(Y)} \right) 
&=\frac{1}{N} \left[
\int   \frac{ h(x)^2 f(x)^2}{g^2(x)} g(x) \, dx  - \left( \int  h(x) \frac{f(x)}{g(x)} g(x) dx \right)^2 \right]
\\
&= E \left[ h(X)^2  \frac{f(X)}{g(X)}\right]  -  E[h(X)]^2
\end{aligned}
$$

. . .

+  We need 
$$
E\left[ h(X)^2  \frac{f(X)}{g(X)}\right]\le E \left[ h(X)^2\right] \iff \frac{f(x)}{g(x)}  \textrm{ "small" when } h(x) \textrm{ is "big"    },.
$$
that is we want to sample more (with $Y$ and pdf $g$ than with $X$ and pdf $f$)  in the region where $h$ is big. 
Hence the name **importance** sampling.

. . .

+ The next result shows what is the best choice of $g$, even if it is practically not very useful since it requires the knowledge of what we are trying to compute in the first place (either $\mu=E[h(X)]$ or $E[|h(X)|]$.) 

## {-}

:::{#thm-optimal.importance.sampling name="Optimal importance sampling"}

1. If $h(x)\ge 0$ then the optimal importance sampling distribution is
$\displaystyle g_*(x) \,=\, \frac{h(x) f(x)}{\mu} = \frac{h(x) f(x)}{E[h(X)]}\,,$
and the corresponding importance sampling estimator $J_N$ has $0$ variance.

2. For general $h$ the optimal importance sampling distribution is
$\displaystyle g_*(x) \,=\, \frac{|h(x)| f(x)}{E[ |h|(X)] }$.
:::

. . .

:::{.proof}  


+ If $h(x) \ge 0$ then  with $g_*(x) \,=\, \frac{h(x) f(x)}{\mu}$ we have $\int g(x) dx =1$ and 
$$
N \textrm{Var} (J_N) =  E \left[ h(X)^2  \frac{f(X)}{g_*(X)}\right]  -  E[h(X)]^2 = E \left[ \frac{h(X)^2 f(X) \mu}{ h(X) f(X) }\right]-  E[h(X)]^2  \,=\, 0 \,.
$$
and thus it is optimal.

+ For general $h$ the optimal variance does not vanish but using Cauchy-Schwartz inequality in the form $E[Z]^2 \le E[Z^2]$ we obtain for any choice of $g$, 
$$
\begin{aligned}
E \left[ h(X)^2 \frac{f(X)}{g_*(X)}\right] & = E \left[ \frac{ h(X)^2  f(X)}{|h|(X) f(X)} \right] E[ |h|(X)] = E \left[ |h|(X) \right]^2 =  E \left[ |h|(Y) \frac{ f(Y) }{g(Y)} \right]^2  \\ & \le E \left[ h(Y)^2  \frac{f(Y)^2}{g(Y)^2}\right] 
=  E \left[ h(X)^2  \frac{f(X)}{g(X)}\right] 
\end{aligned}
$$
and thus $g_*$ gives the optimal variance.

:::


## Examples

+ Suppose we want to sample the mean of standard normal RV $X$ using an importance
sampling  estimator (it may look like a trivial example  but wait...).   
So we take $h(x)=x$ and let us pick $Y$ to be a normal RV with $\mu=0$ and variance $\sigma^2$
$$ 
E\left[h(x)^2 \frac{f(x)}{g(x)}\right] = \int_{-\infty}^\infty x^2 \frac{ (e^{-x^2/2}/\sqrt{2 \pi})^2}{e^{-x^2/2\sigma}/\sqrt{2 \pi} \sigma } \, dx
=  \sigma  \int_{-\infty}^\infty \frac{1}{\sqrt{2 \pi}}  x^2e^{-x^2( 2 - \sigma^{-2})/2} \, dx  
=  \left\{  \begin{array}{cl} \frac{\sigma}{(2-\sigma^{-2})^{3/2}} & \textrm{ if } \sigma^2 > 1/2 \\
+\infty & \textrm{otherwise}  \end{array} \right. 
$$
An elementary computation shows that the optimal choice is $\sigma^2 = 5/2$.

. . . 

+   Suppose $X$ is exponential with paramter $\lambda$  and $h = {\bf I}_{[a,\infty)}$ so that we want to estimate 
$$
\mu= E[ h(X)] = P( X \ge a) = e^{-\lambda a}\,.
$$
Note $e^{-\lambda a}$ maybe very small, e.g. $10^{-5}$. The variance is $e^{-\lambda a}(1- e^{-\lambda a}) \approx e^{-\lambda a}$ and so we need the precision to be at least 
$\varepsilon=10^{-6}$ to have a meaningful estimate. We need then $N = O\left(\frac{\sigma^2}{\epsilon^2}\right)= O(10^7)$....   
The optimal importance sampling is
$$
g_*(x) = \frac{h(x) f(x)}{ e^{-\lambda a}} =  1_{[a,\infty)} \lambda e^{-\lambda(x-a)}
$$
which is a shifted exponential random variable. Note that if sample from $X$ we will mostly obtain $h(X)=0$
and most samples are "useless"  while if we sample from the importance sampling distribution every single
samples contribute a non-zero term to the estimator.



## Tilted distribution


+ **Tilted distributions** are a natural tool for importance sampling. If $f(x)$ is the pdf of $X$ consider the random variable $X^{(t)}$ with the new distribution
$$
f_t(x) = \frac{e^{tx}}{M(t)}f(x) \quad \quad \textrm{ where } M(t)=E[e^{tX}]=\int e^{tx} f(x) dx
$$ 
that is $M(t)$ is the moment generating function of the random variable $X$. We have $M'(t)=E[Xe^{tX}], M''(t)=E[X^2e^{tX}], \cdots$ 

. . .

+ **Example**: If $f(x)=e^{-\lambda x}$ (exponential distribution) then $M(t)=\frac{1}{\lambda -t}$, for $t < \lambda$ 
    and thus 
    $$
    f_t(x)=(\lambda-t) e^{-(\lambda-t)x}\,
    $$
    $X^{(t)}$ an exponential distribution with parameter $\lambda -t$.  
    
. . . 
  
+ **Example**:  If $p(k)=p^k(1-p)^{1-k}$, $k=0,1$ is a Bernoulli distribution then $M(t)=pe^{t} + (1-p)$ and thus 
    $$
    p_t(k)= \frac{e^{tk}}{M(t)}p^k(1-p)^{1-k} = \left(\frac{pe^{t}}{pe^{t} + (1-p)}\right)^k \left(\frac{1-p}{pe^{t} + (1-p)}\right)^{1-k}. 
    $$
    $X^{(t)}$ is again a Bernoulli distribution with success probability $p_t \equiv \frac{pe^{t}}{pe^{t} + (1-p)}$

. . .

+ and so on


## Rare event simulation

+ Suppose $X_1, \cdots X_n$ are IID random variables with joint densities $f(x_1)f(x_2)\cdots f(x_n)$ and $S_n=X_1+\cdots+ X_n$ the sum.  We want to estimate the (high-dimensional integral)
$$
P\left( \frac{S_n}{n} \ge a\right) = E\left[1_{\{S_n \ge n a\}}\right] = \int 1_{\{S_n \ge na\}} f(x_1)f(x_2)\cdots f(x_n) dx_1 \cdots dx_n
$$
If $a > E[X_1]$ this probability is very small (exponentially small in $n$, see @sec-concentration)
and thus the simple Monte-Carlo will fail miserably.

. . . 

+ Tilting the measure to do importance sampling we now have to sample the random variable
$$
1_{\{S_n \ge na\}} \prod_{i=1}^n \frac{f(X^{(t)}_i)}{f_t(X^{(t)}_i)} =
1_{\{S_n \ge na\}} \prod_{i=1}^n \frac{M(t)}{e^{t X^{(t)}_i}} =
1_{\{S_n \ge na\}} e^{-tS_n} M(t)^n \le  
 \left( M(t)e^{-ta}\right)^n
$$

. . .

+ It makes sense to make this random variable as small as possible thus minimize the bound $M(t)e^{-ta}$. Differentiating we find $M'(t)e^{-ta} -  a M(t) e^{-ta}=0$ or 
$$
a = \int x \frac{e^{tx}}{M(t)} f(x) dx = E[X_t], \quad X_t \textrm{ with pdf } f_t(x)
$${#eq-optimal.tilt}
That is we tilt the probability $f$ so that $a$ is the mean of the tilted pdf. So when sampling $X_t$ we should expect to see often values around $a$ making the estimation much easier!

## {-}

+ Numerical example: 

  + $X_i$ is binomial with $p=0.4$, $n=20$ and $a=0.8$. 
  
  + Since $E[X]=p$ for a binomial the equation for the optimal $t$, @eq-optimal.tilt, gives  
  $$
  \frac{pe^{t}}{pe^{t} + (1-p)}  = a  \quad \iff e^{t}= \frac{a}{1-a}\frac{1-p}{p}
  $$
   
  + We find $e^t=6$ and $M(t)=(0.4 e^t +0.6 )= 3$ and thus 
  $$
  1_{\{S_{20} \ge 16\}} \prod_{i=1}^{20} \frac{p(X^{(t)}_i)}{p_t(X^{(t)}_i)}  =1_{\{S_{20} \ge 16\}} \left(\frac{1}{6}\right)^{S_{20}} 3^{20} \le  \left(\frac{1}{6}\right)^{16} 3^{20}=  0.001236
  $$
  
  + We can compute here explicitly that $P(S_{20} \ge 16)=0.000317$ so we have 
  $$
  \textrm{ Simple MC: } \textrm{Var}\left( 1_{\{S_{20} \ge 16\}}\right)= 0.000317(1-0.000317)= 3.169 \times 10^{-4}
  $$
  On the other for the importance sampling the new random variable we sample takes values between $0$ and $0.001236$  and so 
  $$
  \textrm{ IS MC: } \textrm{Var}\left((1_{\{S_{20} \ge 16\}} \left(\frac{1}{6}\right)^{S_{20}} 3^{20}\right) \le \frac{(0.001236)^2}{4}= 3.81 \times 10^{-7}
  $$
  (We have used that $0\le  Y\le a$ implies that $\textrm{Var}(Y) \le \frac{a^2}{4}$, see @sec-concentration for a proof. )
  

## A reliability example  

::::{.columns}

:::{.column width="60%"} 
Each edge fails with probability $q=10^{-2}$ and the edges are independent.  
$\mathbf{X}=(X_1, \cdots, X_{22})$ has pdf (product of Bernoulli)
$$
p(\mathbf{x}) \,\equiv\, P( {\bf X}=  {\bf x}) \,=\, \prod_{i=1}^{22} q^{x_i} (1-q)^{1 - x_i} \,=\, q^{|{\bf x}|} (1-q)^{22 - |{\bf x}|}
$$
with $|x|=\sum_{i=1}^{22}x_i$.  
$h(\mathbf{x})=1$ if there is **no path** from 1 to 22 along working edges and we want $\mu=E[h(X)]$ (failure probability)

:::

:::{.column width="40%"}
  
![Connecting node 1 to node 22](images/graph-22){height=200 #fig-22}

:::

::::

. . . 


+ At least three edges must fail for the system to fail so we have ther bound
$$
\mu \le P(|X| \ge 3) = 1 - \sum_{j=0}^{2} { 22 \choose j} q^j (1-q)^{22-j} = 0.00136 \,.
$$

. . . 

+ To get a lower bound  for $\mu$ by noting that
$\mu \ge P( e_1, e_2, e_3 {\rm ~fail}) = q^3 \,=\, 10^{-6}$.  

+ So the failure probability os between $10^{-6}$ and $10^{-3}$.

## {-}

+ Use importance sampling with $Y$  with 
$q(\mathbf{y}) \,\equiv\, P( {\bf Y}=  {\bf y}) \,=\, \prod_{i=1}^{22} \theta^{y_i} (1-\theta)^{1 - y_i} \,=\, \theta^{|{\bf y}|} (1-q)^{22 - |{\bf y}|}$.

. . .

+ How to pick $\theta$?

. . . 

+ Since we need at least $3$ non-working edges we may want to pick $\theta$ such that the average nnumber of non-working edges is $3$ for $\mathbf{Y}$, that is we choose $\theta=\frac{3}{22}$ (since $|Y|$ is a binomial random variable, $E[|Y|]= 22 \theta$). 

. . . 

+ IS estimator  $J_N = \frac{1}{N} \sum_{i=1}^{N} \frac{ k(\mathbf{Y}_i) p(\mathbf{Y}_i)}{q(\mathbf{Y}_i)}$ with variance
$$
\textrm{Var}(J_N) = \frac{1}{N} \left( \sum_{\mathbf{y}}  \frac{ k(\mathbf{y} )^2 p(\mathbf{y})^2}{q(\mathbf{y})^2} q(\mathbf{y})  -\mu^2 \right) = 
 \frac{1}{N} \left( \sum_{ \mathbf{y}: h({\bf y})=1}  \frac{ p(\mathbf{y})}{q(\mathbf{y})} p(\mathbf{y})  -\mu^2 \right) 
$${#eq-vvest}
Note that
$$
\frac{ p(\mathbf{y}) }{q(\mathbf{y})} \,=\, \frac{q^{| \mathbf{y}|}(1-q)^{22- |\mathbf{y}|}}{\theta^{|\mathbf{y}|}(1-\theta)^{22- |\mathbf{y}|}} =
\left(
\frac{ 1-q}{1-\theta}\right)^{22} \left(\frac{ q (1-\theta) }{\theta(1-q)} \right)^{|\mathbf{y}|}= 20.2 \times (0.064)^{|{\bf y}|}
$$
In @eq-vvest all terms with $k({\bf y})=1$ have $|{\bf y}|\ge 3$ and for those ${\bf y}$ we have
$\frac{ p({\bf y})}{\phi({\bf y})}\,\le\, 20.2 \times (0.064)^{3} \le 0.0053$. So we get
$$
\textrm{Var}(J_N) \,\le\,  \frac{1}{N} \sum_{{\bf y}\,:\, k({\bf y})=1}  0.0053 \,\,p({\bf y}) = \frac{ 0.0053\,\mu}{N}
$$
Reduced the variance by a factor approximately of  $200$ so weed need  $\sqrt{200} \cong 14$ times less samples.


