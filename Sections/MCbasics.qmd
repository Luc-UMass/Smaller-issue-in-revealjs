

We introduce the basic definitions necessary to describe Markov chains and provide a first series of examples. For further reading
we recommend the books @Lawler2006 and @Peres2017.  

## Markov chain on a discrete state space

+ A <span class="red">  **stochastic process** </span> is a sequence of random variables $\{X_0,X_1, X_2,\cdots \}$ where $X_n$ take values in the space $S$, called the <span class="red">**state space** </span> and which we assume for the moment to be discrete (either finite or countable) $S=\{i_1, i_2, \cdots\}$.   Think of the index of $X_n$ as (discrete) "time" and $X_n$ as describing the state of the system at time $n$.  

. . .

+ To describe a general stochastic process we need the joint pdf of $X_0, \cdots, X_n$ (for any $n$) (the finite dimensional distributions)
$$
P \left( X_0= i_0, X_1=i_1, \cdots, X_n = i_n \right)  =  P \left( X_0=i_0\right) P\left( X_1=i_1 \vert X_0=i_0 \right) \cdots P\left( X_n=i_n \vert  X_{n-1}=i_{n-1}, \cdots, X_0=i_0 \right)
$$
which we rewrote in terms of conditional probabilities.   

. . .

+ The <span class="red"> **Markov property** </span> is an assumption on the structure of these conditional probabilities, namely that the future depends only on the present and not on the past, that is     
$$
P\left( X_n=i_n \vert  X_{n-1}=i_{n-1}, \cdots, X_0=i_0 \right)= P(X_n=i_n|X_{n-i}=i_{n-1})
$$
We think here of $n-1$ as the present, and $n$ the future and interpret $P(X_n=j|X_{n-i}=i)$ as the probability to be in state $j$ at time $n$ given that the state at time $n-1$ was $i$.    
Therefore for a Markov process we can rewrite the joint pdf as
$$
P \left( X_0= i_0, X_1=i_1, \cdots, X_n = i_n \right)  \,=\,  P \left( X_0=i_0\right) P\left( X_1=i_1 \vert X_0=i_0 \right) \cdots P\left( X_n=i_n \vert  X_{n-1}=i_{n-1} \right)
$$

. . .

+ We say that a Markov process is <span class="red"> **time-homogeneous** </span> if $P(X_n=j|X_{n-1}=i)$, that is the probability to move from $i$ to $j$ does not depend on the time $n$.  



## {-}

+ A <span class="red"> **time homogeneous Markov chain**  </span> is specified by 

  + Initial distribution $\mu(i)$ for $i \in S$  where $\mu(i)=P(X_0=i)$.   We have 
  $$
  \mu(i)\ge 0 \quad \textrm{ and } \sum_{i \in S} \mu(i) =1
  $$
  and $\mu(i)$ is called a <span class="red"> **probability vector** </span>. 
  
  + Transition matrix  $P(i,j)$ for $i,j \in S$ where $P(i,j)=P(X_n=j|X_{n-1}=i)$. We have 
  $$
  P(i,j) \ge 0 \quad \textrm{ and } \sum_{j \in S} P(i,j)=1
  $$
  and $P(i,j)$ is called a <span class="red"> **transition probabilities** </span>


+ All quantities of interest for the Markov chain can be computed using these two objects.
For example we have
$$
P\left\{ X_0 =i_0, X_1=i_1, X_2=i_2\right\} \,=\, \mu(i_0) P(i_0,i_1) P(i_1, i_2) \,.
$$
or
$$
P\left\{ X_2=i \right\}  \,=\,  \sum_{i_0\in S} \sum_{i_1 \in S} \mu(i_0) P(i_0,i_1) P(i_1, i)
$$
and so on.

## {-}

+ Without loss of generality, we can relabel the states so that
$S=\{ 1, 2, 3, \cdots, N \}$ (with $N\le\infty$).  It will be convenient to set

$$
\mu = ( \mu(1), \mu(2), \cdots, \mu(N))
$$
that is $\mu$ is a row vector whose entries are the initial distribution.   

Also we will write $P$ for the $N \times N$ matrix whose entries are  $P(i,j)$

$$
P \,=\, \left(  \begin{array}{cccc}  P(1,1) & P(1,2) & \cdots & P(1,N) \\   P(2,1) & P(2,2) & \cdots & P(2,N) \\
\vdots & \vdots &  & \vdots \\
P(N,1) & P(N,2) & \cdots & P(N,N) \end{array}
\right)
$$


## Kolmogorov equations 

:::{#prp-Kolmogorovdiscrete name="Kolmogorov equations"}
For a Markov process with initial distribution $\mu$ and transition probabilities $P$: 

1.The $n$-step transition probabilities are given by
$$
P \left\{  X_{n}=j \vert X_0={i} \right\}  \,=\, P^n(i,j)
$$
where $P^n$ is the matrix product $\underbrace{P\cdots P}_{\rm n~times}$.

2.  If $\mu(i)=P\{X_0=i\}$ is the distribution of $X_0$ 
then
$$
P\{ X_n=i\} \,=\, \mu P^n (i)
$$
is the distribution of $X_n$. 


3. If $f = (f(1), \cdots, f(n))^T$ is a column vector (think of $f$ as a function $f: S \to \mathbb{R}$) then we have  
$$
P^n f(i) \,=\, E\left[ f(X_n) | X_0=i\right]. 
$$
:::

:::{.proof} 

For 1. we use induction and assume the  formula is true for $n-1$. 
We condition on the state at time $n-1$ , use the formula $P( AB\vert C) = P(A \vert BC) P(B\vert C)$, the Markov property, 
to find 

:::

## {-}

$$
\begin{aligned}
P \left(  X_{n}=j \vert X_0={i}\right) &= \sum_{k \in S} P \left( X_{n}=j \,, X_{n-1}=k \vert X_0={i} \right)  \\
&= \sum_{k \in S} P \left(  X_{n}=j \vert  X_{n-1}=k \,, X_0={i} \right)  P \left( X_{n-1}=k \vert X_0={i} \right) \\
&= \sum_{k \in S} P \left\{  X_{n}=j \vert  X_{n-1}=k \right\}  P \left\{  X_{n-1}=k \vert X_0={i} \right\} \\
&= \sum_{k \in S} P^{n-1}(i, k) P(k,j)  \,=\, P^n(i,j) \,. 
\end{aligned}
$$


For 2. note that $\mu P$ is a probability vector since
$$
\sum_{i} \mu P(i) \,=\, \sum_{i} \sum_{j} \mu(j ) P(j,i) \,=\, \sum_{j} \mu(j) \sum_i P(j,i) \,=\, \sum_j \mu(j) \,.
$$

Furthermore by the formula for conditional probabilities part 1. 
$$
P\{ X_n=j\} \,=\, \sum_{k \in S} P\{ X_n=j \vert X_0 =k\} P\{ X_0=k\} \,=\,  \sum_{k} \mu(k) P^n(k,j) \,=\, \mu P^n (j)\,.
$$

For 3. we have

$$
P^nf (i) \,=\, \sum_{k} P^n(i,k) f(k) \,=\, \sum_{k} f(k) P \left\{  X_{n}=k \vert X_0={i} \right\} \,=\, E[f(X_{n})\,\vert \, X_0=i] \,.
$$

## Stationary and limiting distributions

+  **Basic question**: For Markov chain  understand the distribution of $\{X_n\}$ for large
 $n$, for example  we may want to know  whether the limit
 $$
 \lim_{n\to \infty} P\{ X_n=i\} \,=\, \lim_{n \to \infty} \mu P^n(i)
 $$
 exists or not, whether it depends on the choice of initial distribution $\mu$ and how to compute it.

. . .

+ A probability vector $\pi$ is called a <span class="red"> **limiting distribution** </span> if the limit
 $\lim_{n \to \infty} \mu P^n = \pi$
 exists. It could well be that the limit depend on the choice of the initial distribution $\mu$.

. . .

+ A probability vector $\pi$ is called a <span class="red"> **stationary distribution**</span> if 
$\pi P = \pi$.

. . .

+ Limiting distributions are always stationary distributions: Suppose $\pi$ is a limiting distribution and 
$\lim_{n\to \infty} \mu P^n = \pi$. Then
$$
\pi P \,=\, (\lim_{n \to \infty} \mu P^n ) P \,=\, \lim_{n \to \infty} \mu P^{n+1}  \,=\, \lim_{n \to \infty} \mu P^{n}
\,=\, \pi \,.
$$
and thus $\pi$ is stationary.  

+ In the next section we will derive conditions under which stationary distributions are unique and are
limiting distributions.

## Examples

:::{#exm-2smc name="2-state Markov chain"}

Suppose $S=\{1,2\}$ with transtion matrix  $\displaystyle P \,=\, \left( \begin{array}{cc} 1-p & p \\ q & 1-q \end{array} \right) \,.$


The equation for the stationary distribution, $\pi P = \pi$ gives 
$$
\pi(1) (1-p) + \pi(2) q \,=\, \pi(1) \quad \quad  
\pi(1) p + \pi(2) (1-q) \,=\, \pi(2)  
$$
that is  $p \pi(1) = q \pi(2)$.  Normalizing to a probability vector gives $\displaystyle \pi = \left( \frac{q}{p+q},\frac{p}{p+q} \right)$.


We show $\pi$ is a limiting distribution. Set $\mu_n \equiv \mu P^n$ and consider the difference $\mu_n -\pi$. Using   $\mu_n(2)=1-\mu_n(1)$ we get the equation
$$
\begin{aligned}
\mu_n(1) - \pi(1) &= \mu_{n-1} P(1) - \pi(1) = \mu_{n-1}(1) (1-p) + (1- \mu_{n-1}(1)) q - \frac{q}{p+q} \\
& = \mu_{n-1} (1)( 1-p-q)  -  \frac{q}{p+q}(1 -p -q) \,=\, (1-p-q) ( \mu_{n-1}(1) - \pi(1) )
\end{aligned}
$$
By induction $\mu_n(1) - \pi(1) = (1-p-q)^n (\mu_0(1) - \pi(1))$. If either $p>0$ or $q>0$ then
$\lim_{n \to \infty} \mu_n(1)  = \pi(1)$ and  this implies that  $\lim_{n\to \infty}  \mu_n(2) = \pi(2)$ as well.

If either $p$ or $q$ does not vanish then $\mu_n = \mu P^n$ converges to a stationary distribution for an arbitrary choice of the initial distribution $\mu$.
 
:::

## {-}

:::{#exm-coupon name="Coupon collecting"}
A company offers toys in breakfast cereal boxes. There are $N$ different toys available and each toy is equally likelky to 
be found in any cereal box.

Let $X_n$ be the number of distinct toys that you collect after buying $n$ boxes and is natural to set
$X_0=0$. Then $X_n$ is a Markov chain with state space $\{0,1,2,\cdots,N\}$ and it has a simple structure since $X_{n}$ either stays the same of increase by $1$ unit.  

The transition probabilities are
$$
\begin{aligned}
P(j, j+1) \,&=\, P\{ {\rm ~new~toy~} \vert {\rm ~already~j~toys}\} \,=\, \frac{N-j}{N} \\ 
P(j, j) \,& =\, P\{ {\rm ~no~new~toy~} \vert {\rm ~already~j~toys}\} \,=\, \frac{j}{N} 
\end{aligned}
$$
The Markov chain $X_n$ will eventually reach the  state $N$ and stays there forever ($N$ is called an absorbing state). 
Let us denote by $\tau$ the  (random) finite time $\tau$ it takes to reach the state $N$.  To compute its expectation, $E[\tau]$, let us write
$$
\tau = T_1 + \cdots +T_N \,,
$$
where $T_i$ is the time needed to get a new toy after you have gotten your $(i-1)^{th}$ toy.  The $T_i$'s
are independent and have $T_i$ has a geometric distribution with $p_i = (N-i)/N$. Thus
$$
E[ \tau] \,=\, \sum_{i=1}^N E[ T_i] \,=\, \sum_{i=1}^N  \frac{N}{N-i} \,=\, N \sum_{i=1}^N  \frac{1}{i} \approx N
\ln (N) \,.
$$
:::

## {-}

:::{#exm-rwg name="Random walk on a graph"}

Consider an *undirected graph*  $G$ consists with *vertex set* $V$ and  *edge set* $E$ (each edge $e=\{v,w\}$ is an (unordered) pair of vertices). We say that the vertex $v$ is a *neighbor* of the vertex $w$, and write  $v \sim w$,  if $\{v,w\}$ is an edge.
The  *degree* of a vertex $v$, denoted $\textrm{deg}(v)$, is the number of neighbor of $v$.

:::


Given such a graph $G=(V,E)$ the *simple random walk* on $G$ is the Markov chain with state space $V$
and transition matrix
$$
P(v,w) \,=\, \left\{ \begin{array}{cl} \frac{1}{{\rm deg} (v)}  & {\rm if~} w \sim v \\ 0 & {\rm otherwise}
\end{array} \right. \,.
$$






The invariant distribution for the random walk on graph is given by $\pi(v) \,=\, \frac{ {\rm deg} (v)} { 2 |E|}$ where is the number of edges.  First note that $\sum_{v} \pi(v)=1$ since each edge connects two vertices. To show invariance note that
$$
\pi P(v) \,=\, \sum_{w} \pi(w)P(w,v) \,=\, \sum_{w ; w \sim v} \frac{{\rm deg} (w)}{2|E|} \frac{1}{ {\rm deg}(w)}
\,=\,  \frac{1}{2|E|}  \sum_{w ; w \sim v}1 \,=\, \pi(v) \,.
$$

:::: {.columns}

::: {.column width="30%"}
![](images/graph-rwg.png){height=1.4in}
:::

::: {.column width="70%"}
$P\,=\, \left( \begin{array}{ccccc}
 0 & \frac13 & \frac13 & \frac 13 & 0 \\
\frac14 & 0 & \frac14 & \frac 14 & \frac14 \\
\frac13 & \frac13 & 0 & 0& \frac 13 \\
\frac13 & \frac13 & 0 & 0 & \frac13 \\
0 & \frac13 & \frac13 & \frac13 & 0 \\
\end{array} \right)$ 
$\quad \quad \quad \pi = \left( \frac{3}{16}, \frac{4}{16}, \frac{3}{16}, \frac{3}{16}, \frac{3}{16}\right)$

:::

::::


## {-}

:::{#exm-hypercube name="Random walk on the hypercube"}
The $d$-dimensional hypercube graph $Q_d$ has for vertices the binary $d$-tuples $\mathbf{x}=(x_1, \cdots, x_d)$ 
with $x_k \in \{0,1\}$ and two vertices are connected by an edge when they differ in exactly one coordinate 
(flipping a $0$ into $1$ or vice-versa). 

The simple random walk on $Q_d$ moves from one vertex $x=(x_1, \cdots, x_K)$
by choosing a coordinate $j \in \{ 1,2,\cdots, d\}$ uniformly at random and setting $x_j \to (1-x_j)$.

The degree of each vertex is $d$, the number of vertices is $2^d$ and the number of edges is
$2^d \frac{d}{2}$ so $\pi(\mathbf{x}) \,=\, \frac{1}{2^d}$,the uniform distribution on $Q_d$.

:::

:::{#exm-rw1N name="Assymetric random walks on {0,1, \cdots, N}"}

State space $S=\{0,1,\cdots,N\}$ and the Markov chain goes up by $1$ with probability $p$ and down by $1$ with probaility $1-p$. 
$$
 P(j, j+1) =p,\, P(j, j-1) =  1-p \,, \quad \textrm {for } j=1, \cdots, N-1
$$
We can pick different boundary conditions (BC) at $0$ and $N$:

+ Absorbing BC: $P(0,0)=1, P(N,N)=1$ 

+ Reflecting BC: $P(0,1)=1, P(N,N-1)=1$
 
+ Partially reflecting BC: $P(0,0)=(1-p)\,, P(0,1)=p P(N,N-1)=(1-p)\,, P(N,N)=p$

+ Periodic BC: $P(0,1)=p\,, P(0, N)=(1-p)\,, \quad P(N, 0) =p\,, P(N, N-1)= (1-p)$

:::

 
## {-}
 
:::{#exm-Ehrenfest name="Ehrenfest urn model"} 

Suppose $d$ balls are distributed among two urns, urn $A$ and urn
$B$.  At each move one ball is selected uniformly at random among the $d$ balls and is transferred
from its current urn to the other urn.  If $X_n$ is the number of balls in urn $A$ then the state space
is $S=\{0, 1, \cdots, d\}$ and the transition probabilities
$$
P(j,j+1) \,=\,  \frac{d-j}{d}\,, \quad P(j,j-1) \,=\,  \frac{j}{d} \,.
$$
We show that the invariant distribution is ninomial with paramters $(d,\frac{1}{2})$, that is  $\pi(j) \,=\, {d \choose j} \frac{1}{2^d}$.  

$$
\begin{aligned}
\pi P (j) &=  \sum_{k} \pi(k) P(k,j) =   \pi(j-1) P(j-1,j) + \pi(j+1) P(j+1, j) \\
             &=  \frac{1}{2^d}\left[  {d \choose j-1} \frac{d - (j-1)}{d} + {d \choose j+1} \frac{j+1}{d} \right] 
             = {d \choose j} \frac{1}{2^d} \,.
\end{aligned}
$$

This Markov chain is closely related to the simple random walk $Y_n$ on the hypercube $Q_d$. Indeed selecting randomly one of the $d$ balls  and moving it the other urn is equivalent to selecting a random  coordinate $y_k$ of $\mathbf{y}=(y_1, \cdots, y_d)$ and changing it to $1-y_k$. If we denote by $j= |\mathbf{y}|=y_1+ \cdots + y_d$ to be the number of $1$s in $\mathbf{y}$. Then 
$$
P(X_n=j+1|X_n=j) = P(\textrm{ choose } k \textrm{ such that } y_k=0) = \frac{d-j}{d}  
$$
:::



