
## Law of large numbers

We start by recalling some basic limit theorems from probability theory.  

- $X_1, X_2, \cdots$ *independent and identically distributed*  random variables (IID). Think of repeating a random experiment $n$ times and recording the results.  We denote by  $S_n = X_1 + X_2 + \cdots + X_n$ the sum.


- The Law of Large numbers shows that if $X_i$ have a finite mean $\mu$ then the empirical average $\frac{S_n}{n}$ converges to $\mu$. 

. . .

:::{#thm-lln  name="Law of Large numbers"}
Suppose that $X_1, X_2, \cdots$ are IID random variables with $E[X_i]=\mu$. 

+ **Weak LLN**: The average  $\frac{S_n}{n}$ converges to $\mu$ in probability: 
$$
\textrm{For any } \varepsilon>0 \quad \lim_{n\to \infty} P\left( \left| \frac{S_n}{n}- \mu\right| \ge \varepsilon \right) =0\,.
$$
+ **Strong LLLN**: The average  $\frac{S_n}{n}$ converges to $\mu$ almost surely. 
$$
\lim_{n\to \infty}\frac{S_n}{n} = \mu \quad \textrm{ with probability } 1
$$

:::

. . .

- Although the strong law of large numbers implies the weak law of large numbers (almost sure convergence implies convergence in probability), they both have their different practical use as we shall see. 


## The Monte-Carlo method


+ The **(simple) Monte-Carlo method** is a probabilistic algorithm using sums of independent random variables 
the law of large numbers to estimate a (deterministic) quantity $\mu \in \mathbb{R}$ (or $\mathbb{R}^d$).   
The basic idea is to express $\mu$  as the expectation of some random variable  $\mu = E [ h(X) ]$
and then use the law of large numbers to build up an estimator for $\mu$.


:::{#thm-MC name="Simple Monte-Carlo Sampling Algorithm"}
To compute $\mu \in \mathbb{R}$ 

+ Find a random variable $h(X)$ such that $\mu= E[h(X)]$. 

+  $I_N =  \frac{1}{N} \sum_{k=1}^N h(X_k)$ is an **unbiased estimator** for $\mu$, that is we have 

   + For all $N$  we have $E[I_N] = \mu$ (unbiased).   
 
   + With probability $1$,  $\lim_{N \to \infty} I_N = \mu$.
  
:::

+ An interesting part is that there are, in general, many ways to find the random variables $h(X)$ and we will play with this, e.g. for importance sampling.   

+ Conversely in many problems the random variable $h(X)$ is given but the expection is too diffcult to compute, so we rely on the LLN to compute $\mu$.





## Buffons needles


:::: {.columns}

::: {.column width="60%"}

:::{.incremental}

+ This seems to be the first example of a rejection sampling used to solve a mathematical problem, by Le Comte de Buffon (see [Bio in Wikipedia](https://en.wikipedia.org/wiki/Georges-Louis_Leclerc,_Comte_de_Buffon)). 

+ A needle of length $l$ is thrown at random on floor made on floorboards of width $d$ and we assume 
$l \le d$. We want to compute the probability that the needle does intersect two floor boards.  

+ Denote by $X$ the distance to the nearest intersection (this is uniformly distributed on $[0,\frac{d}{2}]$)
and by $\Theta$ the acute angle between the needle and an horizontal line( this is uniformly distributed on $[0,\frac{\pi}{2}]$.

+ For the needle to intersect we must have 
$x \le \frac{l}{2} \sin(\theta)$
and thus 
$$
P\left(X \le \frac{l}{2} \sin(\Theta)\right) = \int_0^{\frac{\pi}{2}} \int_0^{\frac{l}{2}\sin(\theta)} \frac{2}{d} dx \, \frac{2}{\pi} d\theta = \frac{2l}{d\pi}
$$
+ So in order estimate $\pi$ you shall throw $N$ needles on the floors at random and  
$$
\pi \approx \frac{2lN}{d} \frac{1}{ \# \textrm { of needles intersecting two floor boards}}
$$
No random number generator needed. 
::: 

:::

::: {.column width="30%"}
![Buffon's needles](images/buffonneedle.png){fig-align="right"}
:::

::::



## Computing $\pi$

+ 
Use the rejection method: Enclose a disk of radius $1$ in a square of side length $2$ and consider the following Bernoulli random variable $X$.

  + Generate 2 independent vectors $V_1,V_2$ uniformly distributed on $[-1,1]$. 
  
  + If $V_1^2 + V_2^2 \le 1$, set $X=1$, otherwise set $X=0$.
  
$X$ is Bernoulli with  $p=\frac{\textrm{area of the disk}}{\textrm{ area of the square}}=\frac{\pi}{4}$ so 
$\frac{4}{N}\sum_{k=1}^N X_i \to \pi$ with probability 1 

```{python}

#| echo: true

import numpy as np
N = 1000000

dart_positions = 2 * np.random.rand(N, 2) - 1  # generates numbers in [-1, 1]
Ncircle = [0] # start the count with 0 to make our loop-logic easier

for x,y in dart_positions:
    if  x**2 + y**2 < 1:
        Ncircle.append(Ncircle[-1] + 1) # another dart has fallen in the circle
    else:
        Ncircle.append(Ncircle[-1])  # the dart fell outside of the circle - Ncircle is unchanged

running_estimate = []
for n_total, n_circle in enumerate(Ncircle[1:]): # skip the inital 0-count
    # n_total starts at 0, so we need to add 1
    running_estimate.append(4 * n_circle / (n_total + 1))

np.transpose(running_estimate[-20:])
```


## Monte-Carlo to compute integrals

+ Goal is to computes integral: $\displaystyle I\,=\, \int_{S} h( \mathbf{x}) d \mathbf{x}$ for some function $h:S\subset \mathbb{R}^d \to R$. 

. . . 

+ To solve an integral by quadrature with a grid with side length $\varepsilon$ you need 
$O\left(\frac{1}{\varepsilon^d}\right)$ grid points ("curse of dimensionality"). 

. . . 

+ Monte Carlo:  Rewrite the integral as an expectation of some random variables you now how to simulate, simulate $N$ of those, and then use the estimator 
$$
\displaystyle I\,=\, \int_{S} h( {\mathbf{x}}) d {\mathbf{x}} = E[g(Y)] \approx I_N=\frac{1}{N}\sum_{k=1}^N g(Y_k)
$$


. . . 


+  If $\displaystyle I_1 \,=\, \int_0^1 \frac{ e^{\sqrt x}-e^{cos(x^3)}}{3 + \cos(x)} \, dx = E[h(U)]$ 
where $U$ is a random number and $h(x)=\frac{ e^{\sqrt x}-e^{cos(x^3)}}{3 + \cos(x)}$

. . . 

+  In general  pick a random variable with pdf $f(x)$ which is non-zero on $S$ and extend $h$ to $\mathbb{R}^d$ by setting it to zero ousisde $S$. Then  
$$
I\,=\, \int_{S} h( \mathbf{x}) d \mathbf{x}\,=\, \int_{\mathbb{R}^d} \frac{h( \mathbf{x})}{f(\mathbf{x})}f(\mathbf{x})  d \mathbf{x} = E\left[ \frac{h( \mathbf{ X})} {f( \mathbf{ X})} \right]
\quad X \textrm{ with pdf } f(x)
$$

. . . 



## Goals

+ Performance gurantees: How many sample should we generate to obtain a given precision for the computation of $\mu = E[h(X)]$ 

  + Use central limit theorems type results to obtain *asymptotic confidence intervals*. 
    
    + Simple and useful but somewhat unsatisfying.
  
  + Use concentration inequalities to obtain *non-asymptotic confidence intervals*. 
    
    + True performance guarantess but harder to get and often too pessimistic. 

. . .

+ How to we design better MC methods since there are many ways.... 


## Central limit theorem

- IID random variables $X_1, X_2, \cdots$ *independent with $\mu=E[X_i]$ and $\sigma^2 = \textrm{Var}(X_i)$.  

. . .

- The central limit theorem tells us about the form fluctuation of the empirical average $\frac{S_n}{n}$ around $\mu$ as $n \to \infty$. Informally 
$$
\frac{S_n}{n} \approx \mu + \frac{\sigma}{\sqrt{n}} Z \quad  \textrm{ as } n\to \infty \quad  \textrm{ with } Z \textrm { a standard normal RV.}
$$

. . .


+ The meaning of $\approx$ is convergence in distribution: 
. . .

:::{#thm-clt name="Central limit theorem"}

Let  $X_1, X_2, \cdots$ be a sequence of independent identically
distributed random variables with mean $E[X_1]=\mu$ and variance
$\textrm{Var}(X_i)=\sigma^2$.  Then for any $- \infty \le a \le b \le \infty$ we have
$$
\lim_{N \to \infty} P \left(  a \le \frac{ S_N - N \mu}{ \sqrt{N} \sigma} \le b \right) 
= \frac{1}{\sqrt{2\pi}} \int_a^b e^{-\frac{x^2}{2}} \, dx \,.
$${#eq-clt}

:::

. . .

+ The CLT is very general (it holds for *any* distribution) and very easy to use. 

+ But is also *asymptotic*, true only as $N\to \infty$. Notot clear how large $N$ we should pick 
for the asymptotic beahvior to be accurate! 

. . .

+ We will build later *non-asymptotic confidence interval*

## Confidence intervals (version 1)


- We build build confidence interval for a Monte-Carlo estimator $I_N=\frac{1}{N}\sum_{k=1}^N h(X_k)$, since  by the Central limit theorem $\frac{\sqrt{N}}{\sigma} ( I_N - \mu)$ is asymptotically normal.  

. . .

- To build a 
$\alpha$-confidence interval we let $z_\alpha$ the number defined by 

$$
\alpha =  \frac{1}{\sqrt{2 \pi}} \int_{-z_\alpha }^{z_\alpha} e^{-\frac{x^2}{2}} \, dx  \quad \quad
 \textrm{ for example  } \quad 
 \left\{\begin{array}{c} 
 z_{.90}=1.645... \,\,(90\% \textrm{ confidence interval}) \\  
 z_{.95}=1.960... \,\,(95\% \textrm{ confidence interval}) \\
 z_{.99}=2.576... \,\,(99\% \textrm{ confidence interval})  
 \end{array} 
 \right.
$$

. . . 

+ By the CLT
$\displaystyle P\left( \mu \in \left[  I_N - z_\alpha \frac{\sigma}{\sqrt{N}}\,,\, I_N + z_\alpha \frac{\sigma}{\sqrt{N}} \right] \right) \preceq \alpha. \quad \textrm{ as } N \to \infty.$

:::{.algorithm}
**Approximate $\alpha$ Confidence Interval**
$$  
P \left( \mu \in [I_N - \epsilon, I_N + \epsilon]  \right) \lessapprox \alpha  \quad 
\textrm{ provided }  \quad 
N \ge   z_\alpha \frac{\sigma^2}{ \epsilon^2} 
$$

:::

+  The issue with that formula is that we are trying to compute $\mu$ so there is no reason to believe that the variance $\sigma^2$ should be known! To remedy this issue we built an estimator for $\sigma^2$ from our samples $X_1, X_2, \cdots$.   

## Central limit theorem with Slutsky theorem

+ Building an estimator for the variance

:::{#thm-varestimator name="Estimator for the variance"}
Given IID random variables $X_1, \cdots, X_N$ with variance $\textrm{Var}(X_i)=\sigma^2$ 
$$
V_N =  \frac{1}{N-1} \sum_{k=1}^N \left(X_k -\frac{S_N}{N}\right)^2
$${#eq-varestimator} 
is an **unbiased estimator** for $\mu$, i.e. $E[V_{N}]=\sigma^2$ and $\lim_{N \to \infty} V_N = \sigma^2$ with probability 1.
:::

. . . 

:::{.proof} 
Details in homework. It relies on the LLN and the following continuity properties: $\frac{S_n}{n}\to \mu$ with prob. 1 and $f$ continuous implies that  $f(\frac{S_n}{n}) \to f(\mu)$.
::: 

+ A theorem on weak convergence (Slutsky's Theorem) implies the following useful version of CLT

:::{#thm-clt2 name="CLT + Slutsky's Theorem"}
Suppose  $X_1, X_2, \cdots$ are IID random variables with $E[X_i]=\mu$ and $\textrm{Var}(X_i)=\sigma^2$.  hen for any $- \infty \le a \le b \le \infty$ we have
$$
\lim_{N \to \infty} P \left(  a \le \frac{ S_N - N \mu}{ \sqrt{N V_N}} \le b \right) 
= \frac{1}{\sqrt{2\pi}} \int_a^b e^{-\frac{x^2}{2}} \, dx \,.
$${#eq-clt2}
:::

## Confidence intervals (version 2)

+ We can get an asymptotic confidence interval using @thm-clt2 

$$
P\left( \mu \in \left[  I_N - z_\alpha \frac{V_N}{\sqrt{N}}\,,\, I_N + z_\alpha \frac{V_N}{\sqrt{N}} \right] \right) \preceq \alpha \,.
$$
or 

:::{.algorithm}
**Approximate $\alpha$ Confidence Interval**
$$  
P \left( \mu \in [I_N - \epsilon, I_N + \epsilon]  \right) \lessapprox \alpha  \quad 
\textrm{ provided }  \quad 
N \ge   z_\alpha \frac{V_N}{ \epsilon^2} 
$$

:::

+ Take home message.

  + Theory: build the Monte-Carlo estimator with the smallest possible variance $long\rightarrow$ Variance Reduction
  
  + For a precision $\varepsilon$ \textrm one needs  $N \sim \frac{1}{\varepsilon^2}$ samples.

  + Practice: With your code always compute both $I_N$ and $V_N$!


## Examples 

 + Compute $\pi$ with accept/reject. Estimator  $I_N=\frac{4}{N} \sum_{k=1}^N {\mathbf{1}}_{\{V_1^2 + V_2^2 \le 1\}}$
 where $V_i$, $i=1,2$ is uniform on $[-1,1]$ so we get 
 $$
 \textrm{Var}(I_N) \,=\, \frac{16}{N}\frac{\pi}{4}\left(1-\frac{\pi}{4}\right)= \pi(4-\pi) = \frac{1}{N} 2.6967 
 $$
 To get $\pi$ with a precision of $\varepsilon=10^{-3}$ and 99% confidence we need at least $N = 2.576 \times 2.6967 \times 10^6= 6.946,699$ samples.  

 + Let $h(x)= \sqrt{1 - x^2}$ for $x \in [0,1]$.  Then $\int_0^1 h(x) dx = \frac{\pi}{4}$ and a estimator for $\pi$     is  $I_N=\frac{16}{N} \sum_{k=1}^N h(U_k)$ with variance
 $$
 \textrm{Var}(I_N)  \,=\, \frac{16}{N} \left( E[h(U)^2] -E[h(U)]^2 \right)= \frac{1}{N}\left(\frac{2}{3}-\left(\frac{\pi}{2}\right)^2\right)= \frac{1}{N} 0.797
 $$
 Better, roughly 4 times less sample (plus using only half as many random numbers).


