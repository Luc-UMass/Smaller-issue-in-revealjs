
## Simulation of random variables 
. . .


+ Problem: How do we generate samples from a random variable X on a computer?  

. . .

+ A computer has a built-in (pseudo-)random number generator. 

```{python}

#| echo: true

# Generating random numbers
import numpy as np
np.random.rand(20)
```

. . .

+ We idealize it as generating independent uniform random variable $U_1, U_2, \cdots$ on the interval $[0,1]$. In practice we have only finite precision and the random number generator is pseudo-random. 

. . .

+ How do we use random numbers to generate other random variables? This problem usually goes under the name of **simulation** of random variables.   

. . .

+ There are a few general principles and lots of clever tricks to generate efficiently specific distributions. 

. . .

+ Good references for simulations are for examples [@Ross2019] and [@Rubinstein2017]

. . . 

+ We give some examples and general in this section and we will add more along the way.

. . . 

## Generating discrete distributions 

:::{#thm-discreterv name="Generating discrete distributions"} 
Suppose $X$ is a discrete random variable taking values $x_1, x_2, x_3, \cdots$ with p.d.f  
$$
p(j) = P(X=x_j)\,.
$$ 
To simulate $X$

+ Generate a random number $U$. 

+ Set 
$$
X\,=\, \left\{ \begin{array} {ll} 
x_1 & \textrm{ if } U < p(1) \\ 
x_2 & \textrm{ if } p(1) \le U < p(1) + p(2) \\ 
\vdots & \vdots \\
x_n & \textrm{ if } p(1) + \cdots + p(n-1) \le U < p(1) + \cdots p(n) \\
\vdots & \vdots \\
\end{array}
\right. 
$$

Then $X$ has the desired distribution. 
:::

. . .

:::{.proof}
This follows simply from $P(a \le U \le b) = b-a$ if $0\le a \le b \le 1$.
:::

## Examples 

+ **Roll a dice**

```{python}

#| echo: true

# Generating the roll of n dice
import numpy as np
n=10
np.ceil(6*np.random.rand(n))
```

. . .

+ **Geometric RV**: If $X$ geometric random variable $X$ with parameter $p$
$$
p(n) =p (1-p)^{n-1} \quad \quad   P(X \le n) = 1 - (1-p)^n \quad n=1,2,\cdots
$$
Then  
$$
1- (1-p)^{n-1} \le U \le 1- (1-p)^{n-1} \iff   (1-p)^n \le (1-U) \le (1-p)^{n-1}
$$
or equivalently since $(1-U)$ has the same distribution as $U$
$$
\textrm{Set } X=n  \quad \textrm{ if } \quad(1-p)^n \le U \le (1-p)^{n-1}
$$

. . . 

+ In general it might be not easy to  compute the CDF $P(X \le n)$ so for specific distributions (say Poisson distributions) we will use special tricks. 


## Inversion method 


:::{#thm-inversionmethod name="Inversion method"}
+ Suppose $X$ is a continuous random variable with CDF $F_X(t)$ and $F_X(t)$ is invertible. Then $X$ and $F_X^{-1}(U)$ have the same distribution.  

+ To simulate $X$ generate a random number $U$ and set $X=F_X^{-1}(U)$. 
:::

. . .

:::{.proof} 
Since $P(U\le a)=a$ for $0\le a \le 1$
$$
P(F_X^{-1}(U)<t) = P(U \le F_X(t)) = F_X(t)
$$
and thus $F_X^{-1}(U)$ has distribution $F_X(t)$.
:::

. . .

+ **Example**: If $T$ has an exponential distribution with paramter $\lambda$ then the CDF is $F(t)=P(T \le t) = 1 -e^{-\lambda t}$ and 
$$
F(t) = 1-e^{-\lambda t}=u \iff t = -\frac{1}{\lambda} \ln(1-u) = F^{-1}(U)
$$
So if $U$ is uniform on $[0,1]$ then $T=-\frac{1}{\lambda} \ln(1-U)$ has an exponential dsitribution with parameter $\lambda$.  Since $U$ and $1-U$ have the same distribution then  $T=-\frac{1}{\lambda} \ln(U)$ has also an exponential distribution with parameter $\lambda$.

. . .

+ Downside: in general $F$ and $F^{-1}$ are not computable (solve them numerically).

## Rejection method 

The next method permits to simulate $X$ provided we can simulate $Y$  ($X$ and $Y$ should have the same support). 

. . .   

:::{#thm-rejectionmethod name="Rejection Method"}
Suppose that $X$ has pdf $f(x)$ and $Y$ has pdf $g(x)$, and that there exists a constant $C$ such that 
$$
\frac{f(y)}{g(y)} \le C \quad \quad  \textrm{ for all } y 
$$
To generate $X$ do          

- Step 1: Generate the random variable $Y$.      

- Step 2: Generate a random number $U$.    

- Step 3: If $U \le \frac{f(Y)}{g(Y)C}$ set $X=Y$ otherwise reject and go back to Step 1.     

The number of times the algorithm runs until a value for $X$ is accepted is a geometric random variable with paramter $\frac{1}{C}$. 
:::

. . . 

:::{.proof}

To obtain one value of $X$ we need iterate the algorithm a random number of times, let us call it  $N$, until the value is accepted.  That is we generate random variables $Y_1, \cdots, Y_N$ until $Y_N$ is accepted and then set $X=Y_N$. 

Let us compute the CDF of $Y_N$. 
:::
## {-}

We have, by conditioning on $Y$ (i.e. $P(A)=\int_y P(A|Y=y)g(y) dy$ for some event $A$).  

$$
\begin{aligned}
P \{ Y_N \le x \}  = P \left\{ Y \le x \,|\, U \le \frac{f(Y)}{C g(Y)}\right\}  
&= \frac{ P \left\{ Y \le x \,, U\le\frac{f(Y)}{C g(Y)}\right\} } 
{P\left\{U \le \frac{f(Y)}{C g(Y)}\right\}}  \\
&= \frac{ \int_{-\infty}^{\infty}  P \left\{ Y \le x  \,,  
U \le \frac{f(Y)}{C g(Y)} 
\,|\, Y=y \right\} g(y) \, dy } {P \left\{ U \le \frac{f(Y)}{C g(Y)}\right\}}
  \\
&= \frac{ \int_{-\infty}^{x} P\left(U\le \frac{f(y)}{C g(y)}\right)g(y)\,dy}
 {P \left(  U \le \frac{f(Y)}{C g(Y)}\right)}  \\
&= \frac{ \int_{-\infty}^{x} \frac{f(y)}{C g(y)}  g(y) \, dy } 
{P \left(  U \le \frac{f(Y)}{C g(Y)}\right)} 
= \frac{ \int_{-\infty}^{x} f(y) \, dy } 
{ C P\left(U \le \frac{f(Y)}{C g(Y)}
\right)}  \,.
\end{aligned}
$$


If we let $x \to \infty$ we obtain that $C P \left( U \le \frac{f(Y)}{C g(Y)}\right)=1$ 
and thus, as desired 
$$
P(Y_N \le x ) \,=\, \int_{-\infty}^x f(x) \,dx \,.
$$
The above argument shows that at each iteration, a value for $X$ is accepted with probability
$P\left( U \le \frac{f(Y)}{C g(Y)} \right) = \frac{1}{C}$ 
independently of the other iterations. Therefore the number of
iterations needed is a geometric random with mean $C$.   




## Example 

Suppose $X$ has pdf $f(x)=20x(1-x)^3$ for $0 \le x\le 1$. Since $X$ is supported on $[0,1]$ we pick $Y$ uniform on $[0,1]$ with $g(y)=1$. Then 
$$
C = \max \frac{f(x)}{g(x)} = \max_{x\in [0,1]} 20x(1-x)^3 = \frac{135}{64}
$$
So generate two random numbers $U_1$ and $U_2$ and if $U_1 \le \frac{256}{27}U_2 (1-U_2)^3$ set $X=U_2$. The average proportion of accepted values is $\frac{64}{135}=.4747..$ 


```{python}
#| echo: true
#| output-location: column
import numpy as np
import matplotlib.pyplot as plt
def accept_reject(N):   # Generate N samples
    n_accept=0
    x_list = [] 
    while n_accept < N:
        a=np.random.rand(2)
        if a[0] < (256/27)*a[1]*(1-a[1])**3 :
            n_accept += 1
            x_list.append(a[1])
    return x_list
plt.figure(figsize=(5,3))  
plt.hist(accept_reject(100000), bins=100, density= 'true')
t = np.arange(0., 1., 0.02)
plt.plot(t, 20*t*(1-t)**3, 'r--' )
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Histogram vs the exact pdf')
plt.show()  
```  


## Normal RV with the rejection method

- We use the fact that we know how to generate an exponential random variable (we will take $\lambda=1$)

. . .

- If $X$ is a standard normal then $\sigma X + \mu$ has mean $\mu$ and variance $\sigma^2$ so it is enough to consider $\mu=0$ and $\sigma^2=1$.

. . .
   
- If $X$ is standard normal then, by symmetry, $X$ and $-X$ have the same distribution and  the random variables 
$Z=|X|$ has the pdf  
$$
f_Z(x) = \frac{2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \quad \textrm{ for } x\ge 0 \,.
$$
   
. . .   
   
- Conversely we can write $X = V Z$ where $V$ is a Rademacher random variable, that is $V$ the RV with pdf  
$P(V= 1) = P(V= -1) = \frac{1}{2}$. For example we can write $V = 2 \lfloor 2U \rfloor -1$ in terms of a random number $U$. 
   
. . .   
   
- Bounding the pdf: 
$$
\displaystyle\max_{x \in [0, \infty)} \frac{ \frac{2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}}{e^{-x}} = \max_{x \in [0, \infty)}  \sqrt{\frac{2e}{\pi}} e^{-\frac{(x-1)^2}{2}} =  \sqrt{\frac{2e}{\pi}} \equiv C=1.3154
$$
   
- Algorithm: generate 3 random numbers $U_1, U_2, U_3$.  If $U_1  \le e^{-\frac{1}{2}(-\ln(U_2)-1)^2}$ set $X= (2 \lfloor 2 U_3 \rfloor -1)(-\ln(U_2))$, otherwise reject.  
   

## Box-Muller algorithm {#sec-BMA}

:::{.incremental}

- Generate 2 independent standard normal using 2 random numbers. 

- Use the change of variable formula. Suppose $X$ and $Y$ are IID standard normal. Then for any (bounded) $h$ 
$$
\begin{aligned}
E[ h(X,Y)] &= \int_{-\infty}^\infty \int_{-\infty}^\infty h(x,y) \frac{1}{2\pi} e^{-\frac{x^2 + y^2}{2}} \, dx \, dy    \\
           &= \int_0^{2\pi} \int_0^\infty h(r \cos(\theta), r \sin(\theta)) r e^{-\frac{r^2}{2}} dr \frac{1}{2\pi} d\theta  \quad \quad     x=r \cos(\theta), y=r \sin(\theta) \\
           &= \int_0^{2\pi} \int_0^\infty h(\sqrt{s} \cos(\theta), \sqrt{s} \sin(\theta)) \frac{1}{2} e^{-\frac{s}{2}} ds \frac{1}{2\pi} d\theta  \quad \quad s=r^2 
\end{aligned}           
$$           
- So if $S$ is exponential with paramter $\frac{1}{2}$ and $\Theta$ is uniform on $[0,2\pi]$ then 
$$
X = \sqrt{S}\cos(\Theta), \quad \quad Y= \sqrt{S} \sin(\Theta)
$$
are 2 independent standard normal random variables.

- We have then a (rejection-free) algorithm: if $U_1$ and $U_2$ are two random numbers then 
$$
X = \sqrt{-2 \ln(U_1)}\cos( 2 \pi U_2), \quad \quad  Y=   \sqrt{-2 \ln(U_1)}\sin( 2 \pi U_2) 
$$
are 2 independent standard normal random variables.

:::

## Generating a random vector on the unit ball

:::{.incremental}

- Rejection method: the unit ball $B_d=\{ x_1^2 + \cdots x_d^2 \le 1\}$ is contained in the cube $C_d =\{ \max_{i} |x_i|\le 1 \}$ 
   
  - Generate $\textbf{Y}=(Y_1, Y_2, \cdots, Y_d)$ uniform on $C_d$ (using $d$ random numbers)
  
  - If $\textbf{Y} \in B_d$, set $\textbf{X}=\textbf{Y}$, otherwise reject and repeat. 

  - We have  
$$
\textrm{ Acceptance probability } =  \frac{\textrm{volume of the sphere}}{\textrm{volume of the cube}}= \frac{\frac{ \pi^{d/2}}{d\Gamma(d/2)}}{2^d} \underbrace{=}_{d=2l}  \frac{\frac{ \pi^{l}}{2l (l-1)!}}{2^{2l}} =  \frac{1}{2 l!}   \left(\frac{\pi}{4}\right)^l  
$$
This is awful, for $d=6$ we have an acceptance probability of $\frac{1}{12} \left(\frac{\pi}{4}\right)^3=0.0403$ so 
$96\%$ of vectors are rejected!  

- A rejection free method: 

  - Generate a uniform distribution on the sphere $S_{d-1}=\{ x_1^2 + \cdots x_d^2 \le 1\}$: Take  $\textbf{ Y}=(Y_1, \cdots, Y_d)$ to be IID standard normal. The joint PDF, $\propto e^{-\frac{1}{2}(x_1^2 + \cdots + x_d^2)}$, is rotation invariant and so $\textbf{ Z}=\frac{\textbf{ Y}}{\|\textbf{ Y}\|}$ is uniformly distributed on $S_{d-1}$. 
 
  - If $\textbf{ X}$ is uniformly distributed on the unit ball, then using spherical coordinates we have for, $r\le 1$, $P(\|\textbf{ X}\|\le r)= \int_0^1 r^{d-1} dr \int_{S_{d-1}} d \sigma = r^d$. 
 
  - Then we claim that if $U$ is a random number then $\textbf{ X}= U^{1/d}Z$ is uniformmyl distributed on the unit ball, since the distribution of $\textbf{ X}$ is rotation invariant and for $r\le 1$
 $$
 P(\|\textbf{ X}\|\le r) =P( U^{\frac1d} \|\textbf{ Z}\| \le r) =P(U \le r^d) =r^d\,. 
 $$

:::


## Generating random permutations

:::{.incremental}


- Our goal is to generate a random permtuation of n elements $(1,2, \cdots, n)$. We write $P=(p(1), p(2), \cdots, p(n))$
for such a permutation. There are $n!$ such permutation which makes enumeration a stupid idea. 

- The following algorithm does the following: after $k$ steps it creates a random permutation of $k$ elements, so you can use it for permutations of any size.  



  - Step 1: Set $k=1$

  - Step 2: Set $p(1)=1$

  - Step 3: If $k=n$ stop. Otherwise set $k=k+1$

  - Step 4: Generate a random number $U$ and set 
$$
p(k) = p( \lceil kU \rceil) \quad \textrm{ and } \quad p(\lceil kU \rceil) = k
$$
then go to step $3$. 


- Since $\lceil kU \rceil$ takes values in $\{1,2,\cdots, k\}$ with probability $\frac{1}{k}$ the algorithm assign the value $k$ randomly to one the $k$ position and assigns the values it replaces to the $k^{th}$ position. 


- By induction, let us assume that after $k-1$ iteration we have obtained a random permutation of $k-1$ elements. 
Then by construction we have
$$
P \left(  i_1, \cdots,  i_{j-1},  k,  i_{j+1}, \cdots, i_{k-1}, i\right)= P\left(i_1, \cdots,  i_{j-1},  i ,  i_{j+1}, \cdots, i_{k-1}\right)\frac{1}{k} = \frac{1}{k!} 
$$

- So we need, for example, 52 steps to create a perfectly shuffled set of cards.

:::


