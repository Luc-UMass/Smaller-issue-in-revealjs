

## Total variation norm 

+ Given two probability measure $\mu$ and $\nu$ on $S$ [the total variation distance between $\mu$ and $\nu$]{.red} is given by 
$$
\|\mu - \nu\|_{\textrm{TV}}= \sup_{A \subset \Omega}|\mu(A) - \nu(A)|
$$
that is the largest distance between the measures of sets.

+ The supremum over all set is not convenient to compute but we have the formula

:::{#thm-tvnorm} 
We have the formula 
$$
\begin{aligned}
\|\mu - \nu\|_{\textrm{TV}} &=  \sum_{i: \mu(i) \ge \nu(i)} (\mu(i) - \nu(i))(x) 
= \frac{1}{2} \sum_{i} | \mu(i) - \nu(i)|
\end{aligned}
$${#eq-tvrep}
:::

:::{.proof} 
First note that the second equality in @eq-tvrep follows from the first. 
Indeed, if the first equality holds, then by interchanging $\mu$ and $\nu$ we also have  
$$
\|\mu - \nu\|_{\textrm{TV}} = 
\sum_{i: \mu(i) \ge \nu(i)} |\mu(i) - \nu(i)| =
\sum_{i: \nu(i) \ge \mu(i)} |\nu(i) - \mu(i)| 
$$
which proves the second equality.
:::

## {-}

To prove the first equality in @eq-tvrep we consider the set $B=\{i:\mu(i) \ge \nu(i)\}$.  For any event $A$ we have 
$$
\begin{aligned} 
\mu(A) - \nu(A) = \sum_{i \in A } \mu(i)-\nu(i) \le \sum_{i \in A \cap B} \mu(i)-\nu(i)  \le \sum_{i \in B} \mu(i)-\nu(i) =\mu(B)-\nu(B)
\end{aligned}
$$
By interchanging the role of $\mu$ and $\nu$ we find 
$$
\begin{aligned}
\nu(A) - \mu(A) &\le \nu(B^c)-\mu(B^c) = \mu(B) -\nu(B)
\end{aligned}
$$
and thus for any set $A$ we have $|\mu(A)-\nu(A)|\le \mu(B) - \nu(B)$.  $\quad\blacksquare$ 

+ The total variation is also intimately related to the notion of coupling between probability measures.  A [coupling between the probability measure $\mu$ and $\nu$]{.red} is a probability measure $q(i,j)$ on
the product space $S \times S$ such that 
$$
\sum_{j} q(i,j) = \mu(j)  \quad \textrm { and } \quad \sum_{j} q(i,j) = \mu(j)
$$
i.e. the marginals of $q$ are $\mu$ and $\nu$. 

+ Coupling are nicely expressed in terms of random variables. We can think of $q(i,j)$ has the (joint) pdf  of the rv $(X,Y)$ where $X$ has pdf $\mu$ and $Y$ has pdf $\nu$. 

+ There always exists a coupling since we can always hoose $X$ and $Y$ independent, i.e. $q(i,j)=\mu(i)\nu(j)$. 

+ On the oppposite extreme if $\mu=\nu$ then we can pick $X=Y$ has a coupling i.e $q(i,i)=\mu(i)=\nu(i)$ and $q(i,j)=0$ if $i\not=j$.

## Total variation and coupling

:::{#thm-tvcoupling name="Coupling representation of total variation"} 
We have 
$$
\|\mu -\nu\|_{\textrm{TV}}= \inf \left\{ P\{X \not =Y\}\,; (X,Y) \textrm{ coupling of } \mu \textrm { and } \nu \right\}
$${#eq-tvcoupling}
:::

:::{.proof} 
We first prove an inequality:
$$
\begin{aligned}
\mu(A)-\nu(A) &= P\{X \in A\} - P\{Y \in A\} \le P\{X \in A\}- P\{X \in A, Y \in A\} 
\\& = P\{X \in A, Y \notin A\} \le P\{ X \not=Y\}
\end{aligned}
$$
and thus $\|\mu-\mu\|_{\textrm{TV}} \le \inf P \{ X \not=Y\}$.  

To show the equality we construct an optimal coupling.  

Recall from @thm-tvnorm that $\displaystyle \|\mu-\nu\|_{\textrm{TV}}=\sum_{\mu(i)\ge \nu(i)}\mu(i)-\nu(i) = \sum_{\nu(i)\ge \mu(i)}\nu(i)-\mu(i)$ (see regions A and B in @fig-coupling) and we set
$$
p= \sum_{i} \mu(i) \wedge \nu(i) = 1 - \|\mu-\nu\|_{\textrm{TV}}
$$
(see region C in @fig-coupling). 

:::

##  {-}



:::{.columns} 

:::{.column width=50%}

![schematics of the optimal coupling](images/coupling.png){#fig-coupling}
:::

:::{.column width=50%}
Consider now the coupling defined as follows. Pick a random number $U$ 

  + If $U \le p$ then let $Z$ be the RV  with pdf 
  $$
  \gamma_C(i)=\frac{  \mu(i) \wedge \nu(i) }{p}
  $$ 
  and set $X=Y=Z$. 

:::

:::
  + If $U>p$ then let $X$ be the random variable with pdf $\gamma_A(i)$ and $Y$ be the random variable with pdf $\gamma_B(i)$ where
    $$
    \begin{aligned}
    \gamma_A(i) =\left\{
      \begin{array}{cl}
      \frac{\mu(i)-\nu(i)}{\|\mu-\nu\|_{\textrm{TV}}} & \mu(i)\ge \nu(i) \\
      0 & \textrm{otherwise}
      \end{array}
      \right.
    \quad   
    \gamma_B(i) =\left\{
      \begin{array}{cl}
      \frac{\nu(i)-\mu(i)}{\|\mu-\nu\|_{\textrm{TV}}} & \nu(i)\ge \mu(i) \\
      0 & \textrm{otherwise}
      \end{array}
      \right.
    \end{aligned}
    $$

Since $p \gamma_C + (1-p) \gamma_A = \mu$ and $p \gamma_C + (1-p) \gamma_B = \nu$ this defines a coupling and we have $P\{ X\not=Y\}=1-p =\|\mu-\nu\|_{\textrm{TV}}$. $\quad \blacksquare$.



## Coupling of Markov chains

+ A [coupling of a Markov chain]{.red} is a stochastic process  $(X_n,Y_n)$ with state space $S\times S$ such that both $X_n$ and $Y_n$ are Markov chains with transition matrix $P$ but with possibly different initial conditions. 

+ A [Markovian coupling]{.red} is a coupling such that the joint $(X_n,Y_n)$ is itself a Markov chain with some transition matrix $Q((i,j), (k,l))$ which must then satisfy
$$
\sum_{k} Q((i,j), (k,l)) = P(j,l) \quad \textrm{ and } \sum_{l} Q((i,j), (k,l)) = P(i,k) \,.
$$

+ Recall that  @thm-convMC where we proved the convergence of aperiodic irreducible Markov chains to their stationary distribution  we used an independent coupling. We will expand upon this idea by considering other more intersting couplings.

+ The [coupling time]{.red} is defined by 
$$
\sigma= \inf \{ n \ge 0 \,:\, X_n=Y_n \}
$$
that is, it is the first time that the Markov chain visit the same state.


+ After the coupling time $\sigma$, $X_n$ and $Y_n$ have the same distribution (by the strong Markov property) so we can always modified a coupling so that, after time $\sigma$  $X_n$ and $Y_n$ are moving together, i.e. $X_n=Y_n$ for all $n \ge \sigma$.  We will *always* assume this to be true in what follows.


## Speed of convergence via coupling 

:::{#thm-couplingMC} 
Suppose $(X_n,Y_n)$ is a coupling of a Markov chain such that $X_0=i$ and $Y_0=j$ and $\sigma$ is the coupling time. Then we have 
$$
\|P^n(i,\cdot) - P^n(j,\cdot)\|_{ \textrm{TV} } \le P \left\{ \sigma>n| X_0=i, Y_0=j\right\}
$$
:::

:::{.proof}
We have $P\{X_n=l|X_0=i\}=P^n(i,l)$ and $P\{Y_n=l|X_0=j\}=P^n(j,l)$ and therefore $X_n$ and $Y_n$ is a coupling of the the probability distributions $P^n(i,\cdot)$ and $P^n(j,\cdot)$. So by @thm-tvcoupling we have 
$$
\|P^n(i,\cdot) - P^n(j,\cdot)\|_{\textrm{TV}} \le P \left\{ X_n \not= Y_n | X_0=i,Y_0=j\right\} = P \left\{ \sigma > n | X_0=i,Y_0=j\right\}\,.
$$
$\blacksquare$
:::

We can use this result to bound the distance to the stationary measure

:::{#thm-boundingTV} 
We have 
$$
\sup_{i \in S } \| P^n(i,\cdot) - \pi\|_{\textrm{TV}} \le \sup_{i,j \in S } \|P^n(i,\cdot) - P^n(j,\cdot)\|_{\textrm{TV}}
$$
:::


## {-}

:::{.proof}
Using the stationarity and the triangle inequality we have 
$$
\begin{aligned}
\| P^n(i,\cdot) - \pi\|_{\textrm{TV}}
&= \sup_{A} |P^n(i,A) - \pi(A)| \\
& = \sup_{A}| \sum_{j} \pi(j) (P^n(i,A)-P^n(j,A))| \\
%& \le \sup_{A} \sum_{j} \pi(j) |P^n(i,A)-P^n(j,A)| \\
& \le  \sum_{j} \pi(j) \sup_{A} |P^n(i,A)-P^n(j,A)| \\
& = \sum_{j} \pi(j) \|P^n(i,\cdot) - P^n(j,\cdot)\|_{TV} \\
& \le \sup_{i,j\in S} \|P^n(i,\cdot) - P^n(j,\cdot)\|_{TV} \quad \blacksquare
\end{aligned}
$$
:::

+ We set  $d(n) = \sup_{i} \|P^n(i,\cdot) - \pi \|_{\textrm{TV}}$ which is the maximal distance to stationarity starting from an arbitrary initial state. (It is not hard to see that $\|\mu P^n -  \pi \|_{\textrm{TV}} \le d(n)$ for abitrary initial distribution $\mu$ as well).     
We define then the  [mixing time $t_{\textrm{mix}}(\varepsilon)$]{.red} of a Markov chain to be 
$$
t_{\textrm{mix}}(\varepsilon) = \min\{ n, d(n) \le \varepsilon \}\,.
$$
That is, if $n>t_{\textrm{mix}}(\varepsilon)$ then $\mu P^n$ is less than $\varepsilon$ close to the stationary distribution.

## Mixing time for the sucess run chain


We start with a countable state space example, the success run chain which is very easy to analyze. 

Parenthetically,  it should be noted that the supremum over $i$ in $d(n)$ is often not well suited for countable state space. It may often happen that the number of steps it take to be close to the stationary distribution may depend on where you start.   

We consider a special case of @exm-successrun with constant succes probability.
$$
P(n,0)=(1-p)\, P(n,n+1)= p\,, n=0,1,2,\cdots
$$

Suppose $X_0=i$ and $Y_0=j$ (with $i\not=j$) we couple the two chains by moving them together.  

  + Pick a random number $U$, if $U \le (1-p)) then set $X_1=Y_1=0$ the couling time $\sigma=1$. 

  + If $U \ge 1-p$ then $move $X_1=i+1$, $Y_1=j+1$. 

Clearly we have 
$$
P( \sigma >n| X_0=i, Y_0=j) = p^n
$$
and thus we find for the mixing time  
$$
d(n)=p^n < \varepsilon \iff  n \ge \frac{\ln(p)}{\ln \varepsilon}
$$


## Mixing time for the random walk on the hypercube

Recall that for  the random walk on the hypercube (see @exm-hypercube) a
state is a $d$-bit $\sigma=(\sigma_1, \cdots, \sigma_d)$ with $\sigma_j\in \{0,1\}$.  

The Markov chain is periodic with period $2$ so to make it aperiodic we 
consider its "lazy" version in which we do not move with probability $1/2$, that is we consider instead the transition matrix $\frac{P+I}{2}$ which makes it periodic. The Markov chain moves then as follows, we pick a coordinate $\sigma_j$ at random and then replace by a random bit $0$ or $1$.


To couple the Markov chain we simply move *move them together*:  If $X_n=\sigma$ and $Y_n=\sigma'$ we pick a coordinate $j \in \{1,\cdots, d\}$ at random  and then replace both $\sigma_j$ and $\sigma'_j$ by the *same* random bit.  This is a coupling and after a move the chosen coordinates coincide.   

Under this coupling $X_n$ and $Y_n$ will get closer to each other if we select a $j$ such that $\sigma_j \not= \sigma'_j$ and we will couple when all the coordinates have been selected.  The distribution of the coupling time is thus exactly the same as the time $\tau$ need to collect $d$ toys in the coupon collector problem.  

We now get a bound on the tail as follows. We denote by $A_i$ the events that coordinate $i$ has not been selected after $m$ steps. We have, 
using the inequality $(1-x)\le e^{-x}$ 
$$
\begin{aligned}
P\{\sigma > n\} = P\left(\bigcup_{i=1}^d A_i\right) \le \sum_{i=1}^d P(A_i) =
\sum_{i=1}^d \left(1- \frac{1}{d}\right)^n \le d e^{-\frac{n}{d}}
\end{aligned}
$$
So if we pick $n = d\ln(d) + c d$ we find 
$$
P\{\sigma > d\ln(d) + c d\} \le d e^{-\frac{d \ln(d) + cd}{d}}= e^{-c} \implies t_{\textrm{mix}}(\varepsilon) \le  d\ln(d) + \ln(\varepsilon^{-1})d\,.
$$


