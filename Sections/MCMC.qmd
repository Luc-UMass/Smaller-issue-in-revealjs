


## MCMC

Suppose you are given a certain probability distribution $\pi$ on a set $S$ and you goal is to
generate a sample from this distribution.   The **Monte-Carlo Markov chain (MCMC)** method consists in
constructing an irreducible Markov chain $X_n$ whose stationary distribution is $\pi$.  Then to generate
$\pi$ one simply runs the Markov chains $X_n$ long enough such that it is close to its
equilibrium distribution.  It turns out that using the detailed balance condition is a very useful tool
to construct the Markov chain in this manner.



A-priori this  method might seems an unduly complicated way to sample from $\pi$. Indeed why not simply
simulate from $\pi$ directly using one of the method of Section [Simulation]

To dispel this impression we will consider some concrete examples but for example we will see that often one want 
to generate a uniform distribution on a set $S$ whose cardinality $|S|$ might be very difficult.

A large class of model are so-called "energy models" which are described by an explicit function $f:S \to \mathbb{R}$ interpreted as the energy (or weight) of the state. Then one is interested in the stationary distribution 
$$
\pi(i) = Z^{-1} e^{-f(i)} \quad \textrm { where } Z= \sum_{i}e^{-f(i)} \textrm{ is a normalization constant}
$$
As we will see computing the normalization constant $Z$ can be very difficult and so one cannot directly simulate from $\pi$! 

The measure $\pi$ assign biggest probability to the states $i$ where $f(i)$ is the smallest, that is they have the smallest energy.  
Often (e.g. in economics) the measure is written with a different sign convention  $\pi(i) = Z^{-1}e^{f(i)}$ which now favor the states with the highest values of $f$. 


## Proper q-coloring of a graph 

For a graph $G=(E,V)$  a [proper $q$-coloring]{.red} consists of assigning to each vertex $v$ of the graph 
one of $q$ colors subject to the constraint that if $2$ vertices are linked by an edge they should have different colors.
The set $S'$ of all  proper $q$-coloring which is a subset of  $S \,=\, \left\{ 1, \cdots, q\right\}^V$.
We denote  the elements of $S$ by $\sigma = \{ \sigma(v)\}_{v \in V}$ with $\sigma(v) \in \{1, \cdots, q\}$.
The uniform distribution on all such proper coloring is $\pi(\sigma) = 1/|S'|$ for all
$\sigma \in S'$.  Even for moderately complicated graph it can be very difficult to compute to $|S'|$.  

Consider the following transition rules:  
1. Choose a vertex $v$ of at random and choose a color $a$ at random.
2. Set  $\sigma'(v) = a$ and $\sigma'(w)=\sigma(w)$ for $w \not=v$.
3. If $\sigma'$ is a proper $q$-coloring then set $X_{n+1}=\sigma'$. Otherwise set $X_{n}=\sigma$.


The transition probabilities are then given by
$$
\begin{aligned}
P(\sigma, \sigma') =\left\{ 
\begin{array}{cl}
\frac{1}{q |V|} &  \textrm{ if } \sigma \textrm{ and } \sigma' \textrm{ differ at exactly one vertex}  \\
 0   & \textrm{ if }\sigma \textrm{ and } \sigma' \textrm{ differ at more than one vertex}  \\
1 - \sum_{\sigma' \not = \sigma} P(\sigma, \sigma') & \textrm{ if } \sigma'=\sigma
\end{array}
\right.
\end{aligned}
$$
Note that $P(\sigma, \sigma)$ is not known explicitly either but is also not used to run the algorithm. The cardinality $|S'|$ is also not needed. 

In order to check that the uniform distribution is stationary for this Markov chain it note that
$P(\sigma, \sigma')$ is symmetric matrix. If one can change $\sigma$ into $\sigma'$ by changing one color
then one can do the reverse transformation too.



## Knapsack problem

This is a classical optimization problem.
You own $m$ books and the $i^{th}$ book has weight $w_i$ lb and is worth \$ $v_i$. In your knapsack you can put at 
most a total of $b$ pounds and you are looking to pack the most valuable knapsack possible.

To formulate the problem mathematically we introduce
$$
\begin{aligned}
w \,&=\, (w_1, \cdots w_m) \in {\bf R}^m   &\textrm{ weight vector}  \\
v \,&=\, (v_1, \cdots v_m) \in {\bf R}^m  &\textrm{ value vector}  \\
\sigma\,&=\, (\sigma_1, \cdots \sigma_m) \in \{0,1\}^m  &\textrm{ decision vector} 
\end{aligned}
$$
where we think that $\sigma_i =1$ if the $i^{th}$ book is in the knapsack.  The state space is
$$
S' \,=\, \left\{   \sigma \in \{0,1\}^{m} \,;\,  \sigma \cdot w \le b \right\}
$$
and the optimization problem is
$$
\textrm{Maximize } v \cdot \sigma \textrm{ subject to the constraint } \sigma \in S' 
$$

As a first step we generate a random element of  $S'$ using the simple algorithm.
If $X_n=\sigma$ then

1. Choose $j \in \{1, \cdots m\}$ at random.
2. Set $\sigma' \,=\, ( \sigma_1, \cdots, 1-\sigma_j, \cdots, \sigma_m)$.
3. If  $\sigma' \in S'$, i.e., if  $\sigma' \cdot w \le b$  then let $X_{n+1}=\sigma'$. Otherwise $X_{n+1}=\sigma$.

## {-}

In other words, choose a random book. If it is in the sack already remove it. If it is not in the sack add it provided you do not
exceed the the maximum weight.  Note that the Markov chain $X_n$ is irreducible, since  each state communicates with the state
$\sigma=(0, \cdots, 0)$.  It is aperiodic  except in the uninteresting case where $\sum_{i} w_i \le b$. Finally the transition probabilities are symmetric and thus the uniform distribution  the unique stationary distribution.


In the knapsack problem  we want to maximize the function $f(\sigma)=\sigma\cdot v$ on the state space.  One possible  algorithm would be to generate an uniform distribution on  the state space and then to look for the maximum value of the function.   But it would be a better idea to sample from a distribution which assign higher probabilities to the states which we are interested in, the ones with a high value of $f$.

Let $S$ be the state space and let $f \,:\, S \to \mathbb{R}$ be a function. It is convenient to introduce the probability distributions define for $\beta>0$ by
$$
\pi_\beta(i) \,=\,  \frac{e^{\beta f(i)}}{Z_\beta} \, \quad \textrm{ with } \quad Z_\beta =\sum_{j \in S} e^{\beta f(j)} \,.
$$
Clearly $\pi_\beta$ assign higher weights to the states $i$ with bigger values of $f(i)$.
Let us define
$$
S^* \,=\, \left\{ i \in S \,;\,  f(i) = f^* \equiv \max_{j \in S} f(j) \right\} \,.
$$
If $\beta=0$ then $\pi_0$ is simply the uniform distribution on $S$. For $\beta \to \infty$ we have
$$
\lim_{\beta \to \infty} \pi_\beta(i) \,=\, \lim_{\beta \to \infty} \frac{ e^{\beta (f(i)- f^*)}} { |S^*| + \sum_{j \in S\setminus S^*} e^{\beta (f(j) - f^* )}} \,=\,
\left\{
\begin{array}{cl}  \frac{1}{|S^*|}  &  {\rm ~if~} i \in S^* \\ 0 & {\rm ~if~} i \notin S^* \\
\end{array}
\right.   \,,
$$
 i.e., for large $\beta$ $\pi_\beta$ is concentrated on the global maxima of $f$.



## Metropolis algorithm

A fairly general method to generate a distribution $\pi$  on the state space $S$ is given the **Metropolis algorithm**. This algorithm assumes that you already know how to generate the uniform distribution on $S$ by using a symmetric transition  matrix $Q$.


:::{#thm-Metropolis name="Metropolis algorithm"}
Let $\pi$ be a probability distribution on $S$ with $\pi(i)>0$ for all $i$ and let $Q$ be a symmetric transition matrix. Consider the Markov chain with the following transition matrix (the Metropolis algorithm). If $X_n=i$ 

1. Choose $Y \in S$ according to $Q$, i.e., $P\{ Y=j \,\vert\, X_n=i\} \,=\, Q(i,j)$
2. Define the acceptance probability $\alpha(i,j) \,=\, \min \left \{ 1 \,, \frac{\pi(j)}{\pi(i)} \right\}$
3. Accept $Y$ with probability $\alpha(i,Y)$ by generating random number $U$. If $U \le \alpha$ then set $X_{n+1}= Y$ (i.e., accept the move) and  if $U > \alpha$ then $X_{n+1}= X_n$ (i.e., reject the move).

If $Q$ is an irreducible transition probability matrix on $S$  then the Metropolis algorithm defines an irreducible Markov chain on $S$  which satisfies detailed balance with stationary distribution $\pi$.
:::

:::{.proof} 
Let $P(i,j)$ be the transition probabilities for the Metropolis Markov chain.  Then we have
$$
P(i,j) \,=\, Q(i,j) \alpha \,=\, Q(i,j) \min \left\{ 1 \,,\, \frac{\pi(j)}{\pi(i)} \right\} \,.
$$

::: 
## {-}  

Since we assume $\pi(i)>0$ for all states $i$, the acceptance probability $\alpha$ never vanishes.  Thus if $P(i,j)>0$ whenever $Q(i,j)>0$ and thus $P$ is irreducible if $Q$ is itself irreducible. 

In order to check the reversibility we note that
$$
\pi(i) P(i,j) \,=\,  Q(i,j) \pi(i) \min \left\{ 1, \frac{\pi(j)}{\pi(i)} \right\} \,=\, Q(i,j) \min \left\{\pi(i)\,,\, \pi(j) \right\}
$$
and the r.h.s is symmetric in $i,j$ and thus $\pi(i) P(i,j) =\pi(j) P(j,i)$.  $\quad \blacksquare$

+ Note that only the ratios $\frac{\pi(j0}{pi(j)}$ are needed to run the algorithm, in particular we do not need the normalization constant. This is a very important feature of the Metropolis algorithm. 


+  We could have chosen another acceptance probability $\alpha(i,j)$. By inspection of the proof it is enough to pick  $\alpha(i,j)$ such that $\alpha(i,j) \le 1$ and  $\pi(i) \alpha(i,j) = \pi(j) \alpha(j,i)$  is symmetric. Some such examples will be considered in the homework and the choice given in @thm-Metropolis is actually optimal in the sense it gives the highest acceptance probability. 

+ The general case with a non-symmetric proposal matrix $Q$ is called the **Metropolis-Hastings algorithm**.  In this case we use the acceptance probability is chosen to be
$$
\alpha(i,j) = \min \left\{ 1, \frac{\pi(j)Q(j,i)}{\pi(i)Q(i,j)} \right\}
$$
and it is not difficult to check (see Homework) that the Metropolis-Hasting algorithm yields a reversible Markov chain with stationary distribution $\pi$.

## Uniform distribution on a graph

Suppose we are given a graph $G=(E,V)$. Often in application to computer networks or social networks the graph is not fully known. Only local information is available, given a vertetx one knows the vertices one is connected. Nonetheless you can run a random walk on the graph by choosing a edge at random and moving to the corresponding vertex, i.e. 
$$
Q(v,w) = \frac{1}{\textrm{ deg}(v)} \textrm{ if } v \sim w
$$
and the stationary distribution is $\pi(v) \propto deg(v)$. 

Suppose you wish to generate a uniform distribution on the graph. Then we can use the Metropolis-Hasting to generate a Markov chain with a uniform stationary distribution.  We have 
$$
\alpha(v,w) = \min\left\{ 1, \frac{Q(w,v)}{Q(v,w)}\right\} =\min\left\{ 1, \frac{\textrm{ deg}(v)}{\textrm{ deg}(w)}\right\}   \textrm{ if } v \sim w
$$
and thus the transition matrix
$$
P(v,w) = Q(v,w) \alpha(v,w) = 
\frac{1}{\textrm{ deg}(v)} \min\left\{ 1, \frac{\textrm{ deg}(v)}{\textrm{ deg}(w)}\right\} 
= \min\left\{ \frac{1}{\textrm{ deg}(v)}, \frac{1}{\textrm{ deg}(w)}   \right\} 
$$
generates a uniform distribution on a graph.

## Metropolis algorithm for the knapsack problem


Consider the distribution  $\pi_\beta(\sigma)  \,=\, \frac{e^{\beta v \cdot \sigma}}{ Z_\beta}$ where the normalization constant $Z_\beta = \sum_{\sigma \in S'} e^{\beta v \cdot \sigma}$ is almost always impossible to compute in practice.  However the ration
$$
\frac{\pi(\sigma')}{\pi(\sigma)} = e^{\beta v \cdot (\sigma'-\sigma)}
$$
does not involve $Z_\beta$ and is easy to compute. 

For this distribution we take as the proposal $Q$ matrix used to generate a uniform distribution on the allowed states of the knapsack (see [Knapsack problem]) and the Metropolis algorithm now reads as follows. 
If $X_n=\sigma$ then

1.  Choose $j \in \{1, \cdots m\}$ at random.
2.  Set $\sigma' \,=\, ( \sigma_1, \cdots, 1-\sigma_j, \cdots, \sigma_m)$.
3.  If  $\sigma'\cdot w >b$ (i.e. if $\sigma \notin S'$ then $X_{n+1}=\sigma$.
4.  If  $\sigma'\cdot w >b$  then let
$\displaystyle  \alpha \,=\, \min \left\{ 1 , \frac{ \pi(\sigma')}{\pi(\sigma)} \right\}  \,=\, \min \left\{1, e^{\beta v \cdot (\sigma' - \sigma)} \right\} \,=\, \left\{ \begin{array}{cl} e^{- \beta v_j} & {\rm ~if~} \sigma_j=1 \\ 1 & {\rm ~if~} \sigma_j=0 \end{array} \right.$
5. Generate a random number $U$, If $U \le \alpha$ then $X_{n+1}=\sigma'$. Otherwise $X_{n+1}=\sigma$.

In short, if  you can add a book to your knapsack you always do so, 
while you remove a book with a probability which is exponentially small 
in the weight. 


## Glauber algorithm


Another algorithm which is widely used for Monte-Carlo Markov chain is the **Glauber algorithm**  which appear in the literature under a variety of other names such as Gibbs sampler in statistical applications, logit rule in economics and social sciences,  heat bath in physics, and undoubtedly under various other names.

The Glauber algorithm is not quite as general as the Metropolis algorithm. Indeed we *assume* that
the state space $S$ has the following structure
$$
S \subset \Omega^V
$$
where both $\Omega$ and $V$ are finite sets.  For example $S \subset \{0,1\}^m$ in the
case of the knapsack problem  or $S \subset  \{ 1, \cdots, q\}^V$  for the case of the proper
$q$-coloring of a graph.   We denote by 
$$
\sigma \,=\, \{ \sigma(v) \}_{v \in  V}   \,, \quad \sigma(v) \in \Omega  \,.
$$
the elements of $S$.

It is useful to introduce the notation
$$
\sigma_{-v} \,=\, \{ \sigma(w) \}_{w \in V, w\not =v}
$$
and we write
$$
\sigma \,=\, ( \sigma_{-v}, \sigma(v) ) \,.
$$
to single out the $v$ entry of the vector $\sigma$.

## {-} 

:::{#thm-Glauber name="Glauber algorithm"}
Let $\pi$ be a probability distribution on $S\subset \Omega^V$ and extend $\pi$ to   $\Omega^V$ by setting $\pi(\sigma)=0$ if
$\sigma \in \Omega^V \setminus S$.  If $X_n \,=\, \sigma$ then

1. Choose $v \in V$ at random.
1.  Replace $\sigma(v)$ by a new value $a \in \Omega$  (provided $( \sigma_{-v}, a) \in S$) with probability
$$
\frac{ \pi( \sigma_{-v}, a) } {\sum_{ b \in \Omega} \pi( \sigma_{-v}, b)} \,.
$$
The Glauber  algorithm defines a  Markov chain on $S$  which satisfies detailed balance with stationary distribution $\pi$.

:::

The irreducibility of the algorithm is not guaranteed a-priori and needs to be checked on
a case-by-case basis.  

:::{.proof} 

The transition probabilities are given by

$$
P(\sigma, \sigma') = 
\left\{ \begin{array}{cl}
  \frac{1}{ |V|}  \frac{ \pi( \sigma_{-v}, \sigma'(v)) } {\sum_{ b \in \Omega} \pi( \sigma_{-v}, b)}  & \textrm{ if } \sigma_{-v} = \sigma'_{-v} {\rm~for~some~}v \\
  0  & \textrm{ if } \sigma_{-v} \not= \sigma'_{-v} {\rm~for~all~}v \\
   1 - \sum_{\sigma'} P(\sigma, \sigma')  & \textrm{ if } \sigma=\sigma'
\end{array} \right.
$$

To check detailed balance we note that if $P(\sigma, \sigma')\not=0$
$$
\pi(\sigma) P(\sigma, \sigma') \,=\,  \frac{\pi(\sigma) \pi(\sigma')} {\sum_{ b \in \Omega} \pi( \sigma_{-v}, b)}  \,,
$$
and this is symmetric in $\sigma$ and $\sigma'$. $\quad \blacksquare$.

::: 


##  Ising model  on a graph  

Let $G=(E,V)$ be a graph and let $S= \{-1,1\}^V$.
That is to each vertex assign the value $\pm 1$, you can think of a magnet at each vertex pointing either
upward ($+1$) or downward ($-1$).   To each $\sigma \in S$ we assign an "energy" $H(\sigma)$ given by
$$
H(\sigma) \,=\,  - \sum_{ e=(v,w) \in E} \sigma(v) \sigma(w) \,.
$$
The energy $\sigma$ is minimal if $\sigma(v) \sigma(w) =1$ i.e., if the magnets at $v$ and $w$ are aligned.
Let us consider the probability distribution
$$
\pi_\beta (\sigma) \,=\, \frac{e^{-\beta H(\sigma)} }{Z_\beta} \,, \quad Z_\beta \,=\,  \sum_{\sigma} e^{-\beta H(\sigma)} \,.
$$
The distribution $\pi_\beta$ is concentrated around the minima of $H(\sigma)$.  To describe the Glauber dynamics note that
$$
H ( \sigma_{-v}, 1) - H( \sigma_{-v}, -1) \,=\,  -2 \sum_{ w\,;\, w \sim v}  \sigma(w)
$$
and this can be computed simply by looking at the vertices connected to $v$ and not at all the graph.   So the transition
probabilities for the Glauber algorithm are given by picking a vertex at random and then updating with probabilities
$$
\frac{ \pi( \sigma_{-v}, \pm 1)} {   \pi( \sigma_{-v}, 1) + \pi( \sigma_{-v},  -1)  } \,=\, \frac{1}{ 1 + e^{\pm \beta \left[ H ( \sigma_{-v}, 1) -
H( \sigma_{-v}, -1)\right]}}   \,=\, \frac{1}{1 + e^{\mp 2 \beta  \sum_{ w\,;\, w \sim v}  \sigma(w)}} \,.
$$

## {-}

By comparison for the Metropolis algorithm we pick a vertex at random and switch $\sigma(v)$ to $-\sigma(v)$
and accept the move with probability
$$
\min\left\{ 1, \frac{ \pi( \sigma_{-v}, - \sigma(v))}{\pi( \sigma_{-v},  \sigma(v))} \right\} \,=\,
\min\left\{ 1, \frac{ \pi( \sigma_{-v}, - \sigma(v))}{\pi( \sigma_{-v},  \sigma(v))} \right\}  \,=\,  \min \left\{ 1, e^{ 2\beta \sum_{ w\,;\, w \sim v}
\sigma(w)\sigma(v)} \right\} \,.
$$



## Simulated annealing  

+ Consider again the problem of finding the minimum of a function $f(j)$, that is 
$$
f^*= \min\{f(j), j \in S\} 
$$
and we denote by $S^*\subset S$ the location of the global minima.
You should think of $f$ as a complicated (and non-convex) function
with complicated level sets and various "local" minima.

. . .

+ To perform this task we sampling a distribution of the form 
$$
\pi_T(j) = e^{-\frac{f(j)}{T}}
$$
which concentrates on the minima of $f(j)$, and the more so as 
$T \to 0$.  

. . .


The idea of simulated annealing comes from physics. The concept of annealing in physics is to obtain a low energy state of a solid (typically a crystal) you first heat it up to reach a liquid state and then, slowly, decrease the temperature to let the particles arrange themselves. 

For a Markov chain the idea is to pick a *temperature schedule*
$$
T_1 > T_2 > T_3 > \cdots \quad \textrm{ with } \lim_{k \to \infty} T_k =0
$$
with $T_1$ sufficiently large.  


## {-}

The following algorithm is a Markov chain with nonstationary transition probabilities.  

:::{.algorithm}
**Simulated annealing algorithm**:  
+ Initialise the Markov chain $X_0$ and the temperature $T_1$.  
+ For each $k$ run $N_k$ steps of the Metropolis or Gibbs sampler with invariant distribution $\pi_{T_k}$.  
+ Update the temperature to $T_{k+1}$ starting with the final configuration.   
:::

A nice result about Metropolis sampler can be found for example in @Hajek1988 (a more precsie version is given there)

:::{#thm-simulatedannealing name="Convergence of simulated annealing"}
For the simulated annealing of the Metropolis algorithm ($N_k=1$) there exists a constant $d^*$ such that we have 
$$
\lim_{n \to \infty} P\{X_n \in S^*\} = 1
$$
if and only if 
$$
\sum_{k=1}^\infty e^{-\frac{d^*}{T_k}} =\infty
$$
The constant $d^*$ measure the depths of the local minima of $f$ where locality is measure in terms of the proposal matrix $Q$. 
:::

## {-}

The theorem tells us that if decrease the temperature very very slowly, e.g. like 
$$
T_k = \frac{c}{\log(k)}
$$
with $c> d^*$ then the Markov chain will converge, with probabilty 1, to a minima.   
This an extremely slow cooling schedule which makes it not very practical 
and other schedules are used like $T_{k+1} = 0.99 T_k$.    


## Parallel tempering


+ The idea of parallel tempering is not use a temperature schedule but rather to use several copies of the system running at different 
temperatures.  The copies at high temperature will explore the state space more efficiently and there is a switching mechanism which 
*exchange* the different copies so that the lower temperature copies can 
take advantage of the exploration done at high temperature. 

. . . 

+ The state space is the product of $k$ copies of $S$
$$
S^{(K)} = \underbrace{S \times \cdots \times S}_{\textrm{k times}}
$$
and we denote a state by the vector $\mathbf{i} = (i_1, \cdots, i_K)$.


+ One picks a set of temperatures $T_1 < T_2 <  \cdots < T_k$ and a probability distribution
$$
\pi^{(k)} ({\bf i}) =\prod_{l=1}^K  \pi_{T_l}(i_l)= \prod_{l=1}^K Z_{l} e^{- \frac{f(i_l)}{T_l}}
$$   
which is the product of the distribution at different temperatures. 


The parallel tempering consists if two kinds of moves. The parallel move 
update each component of $\mathbf{X}_n$ independently with the Metropolis algorithms at different temperature and there is a swapping  move which exchanges a pair of components  of the state vector $\mathbf{X}_n$  in such a way as not to disturb the invariant measure. The component at the lowest temperature  can be used to find the desired minimum.

## {-}


:::{.algorithm} 
**Parallel tempering algorithm**:  
+ Suppose the state $\mathbf{X}=\mathbf{i}$, pick a random number $U$.
+ If $U < \alpha$ then we do the *parallel step* and update each compoent of $\mathbf{X}$ according to a Metropolis move at the corresponding  temperatures $T_l$.  
+ If $U \ge \alpha$ we do the *swapping step*. We randomly chose a neighboring pair $l,l+1$ and propose to swap the components $X_{n,l}$ and 
$X_{n,l+1}$.  We accept the swap with probability
$$
\min\left\{
1, \frac{ \pi_{T_l}( i_{l+1}) \pi_{T_{l+1}}( i_l ) }{ \pi_{T_l}(i_l) \pi_{T_{l+1}}(i_{l+1})} 
\right\}
$$
:::

The parallel moves clearly satisfy satisfy the detailed balance since each component does. 
As for a swap move which swaps the component $i_l$ and $i_{l+1}$ n the state vector, 
we also have detailed balance since 
$$
\begin{aligned}
& \pi_{T_l}(i_l) \pi_{T_{l+1}}(i_{l+1}) (1-\alpha) \frac{1}{K-1}
\min\left\{
1, \frac{ \pi_{T_l}( i_{l+1}) \pi_{T_{l+1}}( i_l ) }{ \pi_{T_l}(i_l) \pi_{T_{l+1}}(i_{l+1})}  
\right\} 
\\
& = (1-\alpha) \frac{1}{K-1}
\min \left\{
 \pi_{T_l}(i_l) \pi_{T_{l+1}}(i_{l+1}) , \pi_{T_l}( i_{l+1}) \pi_{T_{l+1}}( i_l ) 
\right\}
\end{aligned}
$$
which is symmetric in $i_l,i_{l+1}$.   

