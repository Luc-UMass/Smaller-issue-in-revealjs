

## Moment generating function

+ For  a random variable $X$  taking values in $\{0,1,2,3,\cdots\}$ the [generating function of $X$ is the function $\phi_X(s)=E[s^X]$, defined for $s\ge 0$]{.red}. We have 
  $$
  \phi_X(s) = E[s^X] = \sum_{k=0}^\infty s^k P\{X=k\}
  $$
  It is closely related to the moment generating function $E[e^{tX}]$ but this parametrization is more useful her. 

. . .

+ Elementary properties of $\phi_X(s)$: 

    1.  $\phi_X(s)$ is an increasing function of $s$ with $\phi_X(0)=P\{X=0\}$ and $\phi_X(1)=1$.

    2.  Differentiating gives
      $$
      \phi_X'(s) = \sum_{k=1}^\infty k s^{k-1} P\{X=k\}\,, \quad \quad    \phi_X''(s) =\sum_{k=1}^\infty k(k-1) s^{k-2} P\{X=k\}
      $$
      and thus $\phi_X'(1)=E[X]$.  
      If $P\{X \ge 2\}$ then  $\phi_X''(s)>0$ and $\phi_X$ is strictly convex.
    
    3. If $X_1, X_2, \cdots X_m$ are independent random variables then 
      $$
      \phi_{X_1 + \cdots + X_m}(s) = \phi_{X_1}(s) \cdots \phi_{X_1}(s) 
      $$
      
      
## Branching process

+ In a branching process individuals reproduce independently of each other. 

. . .

+ During one time period each indvidual dies and leaves $k$ offsprings with probability $p(k)$ for $k=0,1,2, \cdots$. We denote by $X_n$ the total population after $n$ time period.  

. . .

+ $0$ is an absorbing state and correspond to the extinction of the population. We assume $p(0)>0$ so the absorbing state ban be reached from other states.

. . .

+ The transition probbailities are not easy to write down explicitly. If $X_n=k$ then $X_{n+1}$ will be the sum of the offsprings of the $k$ indviduals. That is 
$$
P\{X_{n+1}=j | X_n=k \} = P\{ Y_1 + \cdots + Y_k =j\}
$$
where $Y_1, \cdots, Y_i$ are IID random variables with pdf $p(k)$.

. . .

+ Mean population $E[X_n]$ is easy to compute by conditioning. Denoting by $\displaystyle \mu=E[Y_1]=\sum_{n=0}^\infty np(n)$ the mean number of offsprings of the individuals we have 
$$
E[X_n|X_{n-1}=k] = E[Y_1 +\cdots + Y_k]=k\mu
$$
and thus 
$$
E[X_n] = E[ E[X_{n}|X_{n-1}]] = \mu E[X_{n-1}] =\cdots = \mu^n E[X_0]
$$


## {-}

+ If $\mu <1$ then $E[X_n] \to 0$ and so the population goes extinct since 
$$
P\{ X_n \ge 1\} = \sum_{k=1}^\infty P\{X_n=k\} \le \sum_{k=0}^\infty k P\{X_n=k\} =E[X_n] \to 0
$$
and so 
$\displaystyle \lim_{n \to \infty} P\{ X_n=0 \} = 1.$

. . .

+ If $\mu=1$ then the population stays constant but it could go extinct with probability $1$ nonetheless (that is $P(X_n=0$ goes to $1$ but $E[X_n]$ is not small).   If $\mu>1$ then the population grows on avergae but it still could go extinct with some non-zero probability. 


. . .

+ To avoid trival cases we  assume that $p(0)>0$ (the population can go extinct) and $p(0)+p(1)< 1$ (the population can grow). We define the [extinction probability]{.red}
$$
\begin{aligned}
a_n(k) &= P\{ X_n=0 | X_0=k\} \\
a(k) & = \lim_{n \to \infty} P\{ X_n=0 | X_0=k\} = P \left\{ \textrm{population goes extinct} | X_0=k \right\}
\end{aligned}
$$

+ Since for the population to go extinct all branches must go extinct, by independence, we have 
$$
a(k) = a(1)^k \quad \textrm{ so we set }  a \equiv a(1)
$$
and we assume from now on that $X_0=1$. Note this formula is also correct for $k=0$ since $a(0)=1$.

## {-}

+ Equation for the extinction probability $a$. By conditioning on the first step
  $$
  \begin{aligned}
  a & = P \left\{ \textrm{population goes extinct} | X_0=1 \right\} \\
    & = \sum_{k=0}^\infty P \left\{ \textrm{population goes extinct} | X_1=k \right\}P\{X_1=k|X_0=1\} \\
    & = \sum_{k=0}^\infty a(k) p(k) \\
    & = \sum_{k=0}^\infty a^k p(k) =  \phi_Z(a)
  \end{aligned}
  $$
  where $\phi_Z(s)$ is the generating function for the offspring distribution $Z$. So we have 
  $$
  \textrm{ The extinction probability solves the fixed point equation } \phi_Z(a)=a \,.
  $$

## {-}

+ Generating function for the population $X_n$. If $X_0=1$, $\phi_{X_0}(s)=s$ and $\phi_{X_1}(s) = \phi_Z(s)$ since $X_1$ are the $Z$ descendents of the single intial individiual.  In addition for $n\ge 2$ we find
  $$
  \begin{aligned}
  \phi_{X_n}(s) &= \sum_{k=0}^\infty P \left\{ X_n= k\right\} s^k \\
                &= \sum_{k=0}^\infty \left[ \sum_{j=0}^\infty P \left\{ X_n= k|X_1=j\right\}P\left\{ X_1=j\right\}\right] s^k \\
                &= \sum_{j=0}^\infty p(j)  \sum_{k=0}^\infty P \left\{ X_n= k|X_1=j\right\} s^k
  \end{aligned}
  $$
  Now note that $X_n$ conditioned on $X_1=j$ is the sum of $j$ independent copies of $X_n$ conditioned on $X_1=1$. Therefore the generating of function $X_n$ conditioned on $X_1=j$ is the generating function of  $X_n$ conditioned on $X_1=1$ to the $j^{th}$ power. So we find 
  $$
  \phi_{X_n}(s)= \sum_{j=0}^\infty p(j) [\phi_{X_{n-1}}(s)]^s = \phi_Z(\phi_{X_{n-1}}(s))
  $$
and thus we get 
$$
\phi_{X_n}(s) = \phi_Z^n(s) = \phi_Z ( \phi_Z( \cdots (\phi_Z(s)))) 
$$


## {-}

Using these computations we are ready to derive the main result

:::{#thm-branchingextinction} 
Let $Z$ be random variable describing the distribution of descendants of a single individual in a branching process and we assume $p(0)>0$ and $p(0)+p(1)<1$.  Then if $X_0=1$ the extinction probability is the smallest root of the equation $\phi_Z(a)=a$. 

1. If $\mu \le 1$ then $a=1$ and the population eventually dies out.

2. If $\mu>1$ then the extinction probability $a<1$ is the unique root of the equation $\phi_Z(s)=s$ with $0 < s < 1$. 

:::

:::{.proof}
We have aready established the extinction probability $a$ is a root of $\phi_Z(s)=s$. But we also know that $1$ is a root since $\phi_Z(1)=1$ and the slope of $\phi_Z$ is equal to $\mu$ at $s=1$.  Since $p(0)+p(1)<1$ then $\phi_Z(s)$ is strictly convex and thus $\phi_Z(s)$ has at most two roots. We have the followoing cases (illustrated in @fig-extinctionprob on the next slide).

1. If $\mu< 1$ the equation $\phi_Z(s)=s$ has two roots $1$ and $s>1$ and the extinction probability is $1$.

2. If $\mu= 1$ the line $s$ is a tangent to the curve $\phi_Z(s)$ at $s=1$ and so  $\phi_Z(s)=s$ has one roots $1$ and 
  the extinction probability is $1$.

3. Finally if $\mu> 1$ the equation $\phi_Z(s)=s$ has two roots $1$ and $a<1$. Since $\phi_Z(0)=p(0)>0$ the second root satisfies  $a>0$. To show that the smallest root is the extinction probability note that we have  
$$
a_n(1)=P\left\{X_n=0|X_0=1\right\}=\phi_Z^n(0)  
$$
:::

## {-}

By induction we show that $a_n(1)\le a$. True for $n=0$ since $a_0(1)=0$. Assuming that $a_{n-1}(1)\le a$ we have  
$$
a_n(1)= P \left\{ X_n=0|X_0=1 \right\}= \phi_Z^n(0) = \phi_Z( \phi_Z^{n-1}(0)) =\phi_Z(a_{n-1}(1)) \le \phi_Z(a) =a
$$
where we used that $\phi_Z$ is increasing.This shows that the smallest root is the extinction probability $\quad \blacksquare$.  


```{python}
#| label: fig-extinctionprob
#| fig-cap: "Extinction probabilities for branching processes"

import numpy as np
import matplotlib.pyplot as plt

x1 = np.linspace(0,4.2, 400)
x2 = np.linspace(0,1.5, 400)
x3 = np.linspace(0,1.1, 400)
y1 = (1/2) + (3/8)*x1 + (1/8)*x1**2
y2=  (3/8) + (1/4)*x2 + (3/8)*x2**2
y3= (1/8) + (1/8)*x3 + (3/4)*x3**2


fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12,4))
#fig.suptitle('Extinction probabilities for the branching process')
ax1.plot(x1, y1)
ax1.plot(x1,x1, color='red')
ax1.set_title('$\mu<1$')
ax1.set(ylabel='$\phi_Z(s)$ and $s$')
ax2.plot(x2, y2)
ax2.plot(x2,x2, color='red')
ax2.set_title(r'$\mu=1$')
ax3.plot(x3, y3)
ax3.plot(x3,x3, color='red')
ax3.set_title('$\mu>1$')
for ax in (ax1, ax2, ax3):
    ax.set(xlabel='s')
plt.show()

```


## Examples

+ If 
$$
p(0)=\frac14, p(1)=\frac14, p(2)=\frac12 \implies \phi(s)=\frac14 + \frac14 s + \frac12 s^2 
$$
then $\mu=\frac{5}{4}>1$ and solving $\phi(a)=a$ gives $a=1,\frac12$ so the extinction probability is $1/2$.

+ If 
$$
p(0)=\frac14, p(1)=\frac12, p(2)=\frac11 \implies \phi(s)=\frac14 + \frac12 s + \frac14 s^2 
$$
then $\mu=1$ and solving $\phi(a)=a$ gives $a=1$ so the extinction probability is $1$.

+ If 
$$
p(0)=\frac12, p(1)=\frac14, p(2)=\frac14 \implies \phi(s)=\frac12 + \frac14 s + \frac14 s^2 
$$
then $\mu=\frac{3}{4}<1$ and solving $\phi(a)=a$ gives $a=1,2$ so the extinction probability is $1$.


## {-}

**Remark**: The theorem provides a numerical algorithm to find the extinction probability (if $\mu <1$). Indeed we have shown that 
the sequence  $0, \phi_Z(0), \phi^2(0), \phi^3(0), \cdots$ converges to $a$. 


```{python}

#| echo: true

import numpy as np
def f(s):
    return (1/8) + (3/8)*s + (1/8)*s**2 + (1/8)*s**3 + (2/8)*s**4

s = 0 # initial condition
N = 20 # the number of iterations

for i in range(N):
    s = f(s)
    print(s)

```




## Repair shop example

Recall the Markov chain in @exm-repairshop. To establish if the system is postive recurrent we try to solve $\pi P=\pi$ and find the system of equations
$$
\begin{aligned}
\pi(0)&= \pi(0)a_0 + \pi(1)a_0 \\
\pi(1)&= \pi(0)a_1 + \pi(1) a_1 + \pi(2)a_0 \\
\pi(2)& = \pi(0)a_2 + \pi(1)a_2 +\pi(2) a_1 + \pi(3)a_0 \\
& \vdots \\
\pi(n)& = \pi(0)a_n + \sum_{j=1}^{n+1} \pi(j) a_{n+1-j} 
\end{aligned}
$$

We solve this using the generating functions $\psi(s)=\sum_{n=0}^\infty s^n \pi(n)$  and $\phi(s)=\sum_{k=0}s^k a_k$: 
$$
\begin{aligned}
\psi(s) = \sum_{n=0}^\infty s^n\pi(n) &= \pi(0) \sum_{n=0}^\infty s^n a_n + \sum_{n=0}^\infty s^n \sum_{j=1}^{n+1} \pi(j) a_{n+1-j}\\
& = \pi(0) \sum_{n=0}^\infty s^n a_n + s^{-1} \sum_{j=1}^\infty \pi(j) s^j \sum_{n=j-1}^\infty s^{n+1-j} a_{n+1-j} \\
&=\pi(0) \phi(s) + s^{-1}(\psi(s)-\pi(0)) \phi(s)
\end{aligned}
$$

## {-}

Solving for $\psi(s)$ we find, after some algebra, the equation 
$$
\psi(s) = \frac{\pi(0) \phi(s)}{1- \frac{1- \phi(s)}{1-s}}
$$

To see if we can find $\pi(0)$ such that this equation canbe solved we take $s \to 1$. We have $\psi(1)=\phi(1)=1$ and we have 
$\displaystyle \lim_{s\to 1}\frac{1- \phi(s)}{1-s}=\phi'(1)=\sum_{k=0}^\infty k a_k=\mu$ which is the mean number of object arriving in the repair shop in a single day.  We find the equation
$$
1= \frac{\pi(0)}{1- \mu}
$$
and so we can find a solution $0 < \pi(0) \le 1$ if and only if $\mu< 1$ and so $\pi(0)=1-\mu$. 
$$
\textrm{ The repair shop Markov chain is positive recurrent iff } \mu < 1
$$

## {-} 

To study transience we use @thm-criteriontransience and try to find a solutinon $P \alpha(j) =\alpha(j)$ for $j\ge 1$ with $\alpha(0)=1$ and $0 < \alpha(j) <1$ for $j>1$. We find the system of equation 
$$
\begin{aligned}
\alpha(1) &= a_0 \alpha(0) + a_1 \alpha(1) + a_2 \alpha(2) + \cdots \\
\alpha(2) &= a_0 \alpha(1) + a_1 \alpha(2) + a_2 \alpha(3) + \cdots \\
\alpha(3) &= a_0 \alpha(2) + a_1 \alpha(3) + a_2 \alpha(4) + \cdots \\
& \vdots  \\
\alpha(n) & = \sum_{j=0}^\infty a_j \alpha(j+n-1)
\end{aligned}
$$
We try for a solution of the form $\alpha(j)=s^j$ which gives 
$$
s^n = \sum_{k=0}^\infty a_j s^{j+n-1} = s^{n-1} \phi(s) \implies \phi(s)=s
$$
and from our analys of branching processes we know that a solution with $s<1$ exists iff $\mu= \sum_{k=0}^\infty k a_k >1$. So we get 
$$
\textrm{ The repair shop Markov chain is transient iff } \mu > 1
$$
and it is null-recurrent if $\mu=1$. 
